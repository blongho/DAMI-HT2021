{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2: Unsupervised Learning - Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this homework you will work with Unsupervised learning and cluster a real-world dataset. You will apply 4 different algorithms, plot and evaluate the clustering results. Finally you will implement K-medians from scratch!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Student information**\n",
    "Please provide your information for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "STUD_SUID = 'lobe2042'\n",
    "STUD_NAME = 'Longho Bernard Che'\n",
    "STUD_EMAIL = 'lobe2042@student.su.se'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Grading: \n",
    "\n",
    "Total points: 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OUTLINE\n",
    "## Total points: 5\n",
    "\n",
    "1. Euclidean - Manhattan , **points: 0.5**\n",
    "2. Purity , **points: 0.6**\n",
    "3. Read and preprocess the wine dataset, **points: 0.2**\n",
    "4. Elbow method, **points: 0.7**\n",
    "5. Run K-means, K-medoids and Agglomerative clustering on the wine dataset, **points: 0.4**\n",
    "6. Evaluation metrics, **points: 0.3**\n",
    "7. Plotting, **points: 0.3**\n",
    "6. Implementation of K-medians, **points: 2**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `NOTE: Each function you make will be graded, so it is important to strictly follow input and output instructions stated in the skeleton code. Some of the cells have already some variables that are filled with None values or empty dataframes, you should change those nan/empty values (we refer to it as 'change this') to what is asked in the tasks (we only stored the empty values so the whole notebook can run error free). You should not delete any of the given cells as they will help us grade the assignment. Some cells ask you to uncomment some comments, please only do so if you have solved the respective task. When you are finished with implementing all the tasks, clear all outputs,  **restart the kernel**, run all cells again (make sure there is no error) and submit! Make sure that the results and figures asked are visible for us to grade. ` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset that you will use in this assignment is the Wine Dataset which you will download using sklearns dataset module (instructions are provided below in Task3):\n",
    "\n",
    "Information can be found in the [link](https://archive.ics.uci.edu/ml/datasets/wine)\n",
    "\n",
    "The data is the results of a chemical analysis of wines grown in the same region in Italy by three different cultivators. There are thirteen different measurements taken for different constituents found in the three types of wine.\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "    - Alcohol\n",
    "    - Malic acid\n",
    "    - Ash\n",
    "    - Alcalinity of ash\n",
    "    - Magnesium\n",
    "    - Total phenols\n",
    "    - Flavanoids\n",
    "    - Nonflavanoid phenols\n",
    "    - Proanthocyanins\n",
    "    - Color intensity\n",
    "    - Hue\n",
    "    - OD280/OD315 of diluted wines\n",
    "    - Proline\n",
    "\n",
    "All attributes are numerical. The are no missing values. \n",
    "\n",
    "The target/class is the 3 different types of wines. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import here all the libraries needed for this assignment\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "RSEED = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Euclidean-Manhattan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Task:`\n",
    "\n",
    "###  `In the function named calculate_distances: `\n",
    "###  `- compute the Euclidean distance between any two given vectors (numpy arrays)`\n",
    "\n",
    "The euclidean distance:\n",
    "$d_e = \\sqrt{\\sum_{i=1}^{n}({a_i-b_i})^2}$\n",
    "\n",
    "###  `- compute the Manhattan distance between any two given vectors (numpy arrays)`\n",
    "\n",
    "The manhattan distance: \n",
    "$d_m = \\sum_{i=1}^{n}{\\lvert a_i-b_i \\rvert}$ \n",
    "\n",
    "**Important**: You should implement this yourself from scratch (you could use numpy). If you call a method to calculate the distances the task will get no points. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distances(vector1, vector2, name_of_distance_metric):\n",
    "\n",
    "    \"\"\"\n",
    "    Input:\n",
    "            vector1: numpy array \n",
    "            vector2: numpy array \n",
    "            name_of_distance_metric: string taking values: 'euclidean' or 'manhattan'\n",
    "          \n",
    "    Output:\n",
    "            distance: the value of the distance that you calculated\n",
    "    \n",
    "\n",
    "    step 1: if the name of the distance metric is \"euclidean\" then store in the output variable the result of the euclidean distance between vector1 and vector2\n",
    "    step 2: if the name of the distance metric is \"manhattan\" then store in the output variable the result of the manhattan distance between vector1 and vector2\n",
    "        \n",
    "    Note: implement this yourself from scratch! if you want to make sure that the calculations are correct, cross check it by calculating the manhattan/euclidean distances by\n",
    "    hand or by calling a method that does these calculations, to see what's the correct result. Of course you should not include these checks in the function but your from-scratch implementation!\n",
    "    \"\"\"\n",
    "    distance = 0\n",
    "    if name_of_distance_metric == \"euclidean\":\n",
    "        sum_of_squared_difference = 0\n",
    "        for idx, _ in enumerate(vector1):\n",
    "            sum_of_squared_difference += ((vector1[idx] - vector2[idx])**2)\n",
    "        distance = math.sqrt(sum_of_squared_difference)\n",
    "\n",
    "    if name_of_distance_metric == \"manhattan\":\n",
    "        sum_of_absolute_difference = 0\n",
    "        for idx, _ in enumerate(vector1):\n",
    "            sum_of_absolute_difference += (abs(vector1[idx] - vector2[idx]))\n",
    "        distance = sum_of_absolute_difference\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test your function here using these two arrays (do not change them!).\n",
    "# do not delete this!\n",
    "v1 = np.array((1, 2, 3, 4, 5))\n",
    "v2 = np.array((7, 8, 9, 4, 2))\n",
    "euclidean = calculate_distances(v1,v2, \"euclidean\")\n",
    "manhattan = calculate_distances(v1,v2, \"manhattan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "21"
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do not delete this!\n",
    "manhattan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "10.816653826391969"
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do not delete this!\n",
    "euclidean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Purity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Task:  In the function named purity calculate the purity metric for any given clustering result. `\n",
    "\n",
    "Read more about purity [here](https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html)\n",
    "\n",
    "\n",
    "You will use the purity function later on the evaluation of the clustering results of the wine dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hints:\n",
    "\n",
    "To calculate Purity first create a contigency matrix (you can call the sklearn's version of the contigency matrix [link](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cluster.contingency_matrix.html#sklearn.metrics.cluster.contingency_matrix)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 3],\n       [1, 1]])"
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is an example of how a contigency matrix works:\n",
    "# suppose that this array holds your ground truth labels\n",
    "true = ['a', 'b', 'a', 'b', 'a'] # so we have two groupings that belong to 'a' and 'b'. Sample 0 belongs to group 'a', sample 1 belongs to group 'b' and so on.\n",
    "# suppose that we did some clustering and that this array holds your predicted labels from a clustering algo\n",
    "predicted = [1, 1, 1, 0, 1] # so samples 0 to 2 and sample 4 belong to cluster 1, sample 3 to cluster 0 according to some clustering algorithm\n",
    "\n",
    "# let's calculate the contigency matrix for the above\n",
    "metrics.cluster.contingency_matrix(['a', 'b', 'a', 'b', 'a'], [1, 1, 1, 0, 1])\n",
    "\n",
    "# The first row of output array indicates that there are 3 samples whose true cluster is “a”.\n",
    "# Of them, None are predicted in cluster 0, and 3 are predicted in cluster 1.\n",
    "# And the second row indicates that there are 2 samples whose true cluster is “b”. Of them, 1 is predicted in cluster 0, and 1 is predicted in cluster 1.\n",
    "# If you'd like you could use the result of the contigency matrix to calculate purity! check the link above or the lecture material to see how purity is calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purity(y_true, y_pred):\n",
    "\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        y_true: numpy array, the true labels of your dataset\n",
    "        y_pred: numpy array, the labels predicted by the algorithm\n",
    "    Output:\n",
    "        purity: the resulting value for purity\n",
    "\n",
    "\n",
    "    step 1: create the contigency matrix for the true and predicted labels\n",
    "    step 2: find the max value of correctly assigned samples in the cluster (each cluster is assigned to the class which is most frequent in the cluster)\n",
    "    step 3: sum the correcly assigned samples and divide by the sum of points in the contigency matrix\n",
    "    \"\"\"\n",
    "    contingency_matrix = metrics.cluster.contingency_matrix(labels_true=y_true, labels_pred=y_pred)\n",
    "    max_correctly_assigned_values = np.amax(contingency_matrix, axis=0)\n",
    "    purity = np.sum(max_correctly_assigned_values/np.sum(contingency_matrix))\n",
    "\n",
    "    return purity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.8181818181818181"
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test your function here using these two arrays (do not change them!).\n",
    "# do not delete this!\n",
    "ground_truth = pd.Series([1,1,2,1,2,1,0,1,1,0,0])\n",
    "predicted_labels = pd.Series([1,0,2,1,2,1,0,1,1,2,0])\n",
    "\n",
    "purity(ground_truth, predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Read and preprocess the wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n0    14.23        1.71  2.43               15.6      127.0           2.80   \n1    13.20        1.78  2.14               11.2      100.0           2.65   \n2    13.16        2.36  2.67               18.6      101.0           2.80   \n3    14.37        1.95  2.50               16.8      113.0           3.85   \n4    13.24        2.59  2.87               21.0      118.0           2.80   \n\n   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n0        3.06                  0.28             2.29             5.64  1.04   \n1        2.76                  0.26             1.28             4.38  1.05   \n2        3.24                  0.30             2.81             5.68  1.03   \n3        3.49                  0.24             2.18             7.80  0.86   \n4        2.69                  0.39             1.82             4.32  1.04   \n\n   od280/od315_of_diluted_wines  proline  class  \n0                          3.92   1065.0      0  \n1                          3.40   1050.0      0  \n2                          3.17   1185.0      0  \n3                          3.45   1480.0      0  \n4                          2.93    735.0      0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>alcohol</th>\n      <th>malic_acid</th>\n      <th>ash</th>\n      <th>alcalinity_of_ash</th>\n      <th>magnesium</th>\n      <th>total_phenols</th>\n      <th>flavanoids</th>\n      <th>nonflavanoid_phenols</th>\n      <th>proanthocyanins</th>\n      <th>color_intensity</th>\n      <th>hue</th>\n      <th>od280/od315_of_diluted_wines</th>\n      <th>proline</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>14.23</td>\n      <td>1.71</td>\n      <td>2.43</td>\n      <td>15.6</td>\n      <td>127.0</td>\n      <td>2.80</td>\n      <td>3.06</td>\n      <td>0.28</td>\n      <td>2.29</td>\n      <td>5.64</td>\n      <td>1.04</td>\n      <td>3.92</td>\n      <td>1065.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>13.20</td>\n      <td>1.78</td>\n      <td>2.14</td>\n      <td>11.2</td>\n      <td>100.0</td>\n      <td>2.65</td>\n      <td>2.76</td>\n      <td>0.26</td>\n      <td>1.28</td>\n      <td>4.38</td>\n      <td>1.05</td>\n      <td>3.40</td>\n      <td>1050.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13.16</td>\n      <td>2.36</td>\n      <td>2.67</td>\n      <td>18.6</td>\n      <td>101.0</td>\n      <td>2.80</td>\n      <td>3.24</td>\n      <td>0.30</td>\n      <td>2.81</td>\n      <td>5.68</td>\n      <td>1.03</td>\n      <td>3.17</td>\n      <td>1185.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>14.37</td>\n      <td>1.95</td>\n      <td>2.50</td>\n      <td>16.8</td>\n      <td>113.0</td>\n      <td>3.85</td>\n      <td>3.49</td>\n      <td>0.24</td>\n      <td>2.18</td>\n      <td>7.80</td>\n      <td>0.86</td>\n      <td>3.45</td>\n      <td>1480.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>13.24</td>\n      <td>2.59</td>\n      <td>2.87</td>\n      <td>21.0</td>\n      <td>118.0</td>\n      <td>2.80</td>\n      <td>2.69</td>\n      <td>0.39</td>\n      <td>1.82</td>\n      <td>4.32</td>\n      <td>1.04</td>\n      <td>2.93</td>\n      <td>735.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the wine dataset from sklearn\n",
    "wine = datasets.load_wine()\n",
    "\n",
    "data = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "\n",
    "# the column class is your target/class. The seperation of samples to the 3 different wine cultivators\n",
    "data['class'] = wine.target\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 178 entries, 0 to 177\n",
      "Data columns (total 14 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   alcohol                       178 non-null    float64\n",
      " 1   malic_acid                    178 non-null    float64\n",
      " 2   ash                           178 non-null    float64\n",
      " 3   alcalinity_of_ash             178 non-null    float64\n",
      " 4   magnesium                     178 non-null    float64\n",
      " 5   total_phenols                 178 non-null    float64\n",
      " 6   flavanoids                    178 non-null    float64\n",
      " 7   nonflavanoid_phenols          178 non-null    float64\n",
      " 8   proanthocyanins               178 non-null    float64\n",
      " 9   color_intensity               178 non-null    float64\n",
      " 10  hue                           178 non-null    float64\n",
      " 11  od280/od315_of_diluted_wines  178 non-null    float64\n",
      " 12  proline                       178 non-null    float64\n",
      " 13  class                         178 non-null    int64  \n",
      "dtypes: float64(13), int64(1)\n",
      "memory usage: 19.6 KB\n"
     ]
    }
   ],
   "source": [
    "# if you want to see information about the dataset\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "          alcohol  malic_acid         ash  alcalinity_of_ash   magnesium  \\\ncount  178.000000  178.000000  178.000000         178.000000  178.000000   \nmean    13.000618    2.336348    2.366517          19.494944   99.741573   \nstd      0.811827    1.117146    0.274344           3.339564   14.282484   \nmin     11.030000    0.740000    1.360000          10.600000   70.000000   \n25%     12.362500    1.602500    2.210000          17.200000   88.000000   \n50%     13.050000    1.865000    2.360000          19.500000   98.000000   \n75%     13.677500    3.082500    2.557500          21.500000  107.000000   \nmax     14.830000    5.800000    3.230000          30.000000  162.000000   \n\n       total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\ncount     178.000000  178.000000            178.000000       178.000000   \nmean        2.295112    2.029270              0.361854         1.590899   \nstd         0.625851    0.998859              0.124453         0.572359   \nmin         0.980000    0.340000              0.130000         0.410000   \n25%         1.742500    1.205000              0.270000         1.250000   \n50%         2.355000    2.135000              0.340000         1.555000   \n75%         2.800000    2.875000              0.437500         1.950000   \nmax         3.880000    5.080000              0.660000         3.580000   \n\n       color_intensity         hue  od280/od315_of_diluted_wines      proline  \ncount       178.000000  178.000000                    178.000000   178.000000  \nmean          5.058090    0.957449                      2.611685   746.893258  \nstd           2.318286    0.228572                      0.709990   314.907474  \nmin           1.280000    0.480000                      1.270000   278.000000  \n25%           3.220000    0.782500                      1.937500   500.500000  \n50%           4.690000    0.965000                      2.780000   673.500000  \n75%           6.200000    1.120000                      3.170000   985.000000  \nmax          13.000000    1.710000                      4.000000  1680.000000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>alcohol</th>\n      <th>malic_acid</th>\n      <th>ash</th>\n      <th>alcalinity_of_ash</th>\n      <th>magnesium</th>\n      <th>total_phenols</th>\n      <th>flavanoids</th>\n      <th>nonflavanoid_phenols</th>\n      <th>proanthocyanins</th>\n      <th>color_intensity</th>\n      <th>hue</th>\n      <th>od280/od315_of_diluted_wines</th>\n      <th>proline</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n      <td>178.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>13.000618</td>\n      <td>2.336348</td>\n      <td>2.366517</td>\n      <td>19.494944</td>\n      <td>99.741573</td>\n      <td>2.295112</td>\n      <td>2.029270</td>\n      <td>0.361854</td>\n      <td>1.590899</td>\n      <td>5.058090</td>\n      <td>0.957449</td>\n      <td>2.611685</td>\n      <td>746.893258</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.811827</td>\n      <td>1.117146</td>\n      <td>0.274344</td>\n      <td>3.339564</td>\n      <td>14.282484</td>\n      <td>0.625851</td>\n      <td>0.998859</td>\n      <td>0.124453</td>\n      <td>0.572359</td>\n      <td>2.318286</td>\n      <td>0.228572</td>\n      <td>0.709990</td>\n      <td>314.907474</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>11.030000</td>\n      <td>0.740000</td>\n      <td>1.360000</td>\n      <td>10.600000</td>\n      <td>70.000000</td>\n      <td>0.980000</td>\n      <td>0.340000</td>\n      <td>0.130000</td>\n      <td>0.410000</td>\n      <td>1.280000</td>\n      <td>0.480000</td>\n      <td>1.270000</td>\n      <td>278.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>12.362500</td>\n      <td>1.602500</td>\n      <td>2.210000</td>\n      <td>17.200000</td>\n      <td>88.000000</td>\n      <td>1.742500</td>\n      <td>1.205000</td>\n      <td>0.270000</td>\n      <td>1.250000</td>\n      <td>3.220000</td>\n      <td>0.782500</td>\n      <td>1.937500</td>\n      <td>500.500000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>13.050000</td>\n      <td>1.865000</td>\n      <td>2.360000</td>\n      <td>19.500000</td>\n      <td>98.000000</td>\n      <td>2.355000</td>\n      <td>2.135000</td>\n      <td>0.340000</td>\n      <td>1.555000</td>\n      <td>4.690000</td>\n      <td>0.965000</td>\n      <td>2.780000</td>\n      <td>673.500000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>13.677500</td>\n      <td>3.082500</td>\n      <td>2.557500</td>\n      <td>21.500000</td>\n      <td>107.000000</td>\n      <td>2.800000</td>\n      <td>2.875000</td>\n      <td>0.437500</td>\n      <td>1.950000</td>\n      <td>6.200000</td>\n      <td>1.120000</td>\n      <td>3.170000</td>\n      <td>985.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>14.830000</td>\n      <td>5.800000</td>\n      <td>3.230000</td>\n      <td>30.000000</td>\n      <td>162.000000</td>\n      <td>3.880000</td>\n      <td>5.080000</td>\n      <td>0.660000</td>\n      <td>3.580000</td>\n      <td>13.000000</td>\n      <td>1.710000</td>\n      <td>4.000000</td>\n      <td>1680.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[:,:-1].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Task: Standardize all the numerical features. You can use the StandardScaler from sklearn`\n",
    "**Important**: Remember that the class/target should not be standardized!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 1: Initialize a StandardScaler object\n",
    "Step 2: fit_transform the values of the numerical features (not the class!)\n",
    "Step 3: Transform the standardized returned array into a dataframe called **data_standardized** with the corresponding column names\n",
    "\"\"\"\n",
    "columns_without_class = data.columns[:-1]\n",
    "data_without_class = data[columns_without_class]\n",
    "\n",
    "data_standardized = pd.DataFrame(data_without_class, columns=columns_without_class)\n",
    "\n",
    "scaled_data = StandardScaler().fit_transform(data_standardized)\n",
    "\n",
    "data_standardized = pd.DataFrame(scaled_data, columns=columns_without_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "    alcohol  malic_acid       ash  alcalinity_of_ash  magnesium  \\\n0  1.518613   -0.562250  0.232053          -1.169593   1.913905   \n1  0.246290   -0.499413 -0.827996          -2.490847   0.018145   \n2  0.196879    0.021231  1.109334          -0.268738   0.088358   \n3  1.691550   -0.346811  0.487926          -0.809251   0.930918   \n4  0.295700    0.227694  1.840403           0.451946   1.281985   \n\n   total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\n0       0.808997    1.034819             -0.659563         1.224884   \n1       0.568648    0.733629             -0.820719        -0.544721   \n2       0.808997    1.215533             -0.498407         2.135968   \n3       2.491446    1.466525             -0.981875         1.032155   \n4       0.808997    0.663351              0.226796         0.401404   \n\n   color_intensity       hue  od280/od315_of_diluted_wines   proline  \n0         0.251717  0.362177                      1.847920  1.013009  \n1        -0.293321  0.406051                      1.113449  0.965242  \n2         0.269020  0.318304                      0.788587  1.395148  \n3         1.186068 -0.427544                      1.184071  2.334574  \n4        -0.319276  0.362177                      0.449601 -0.037874  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>alcohol</th>\n      <th>malic_acid</th>\n      <th>ash</th>\n      <th>alcalinity_of_ash</th>\n      <th>magnesium</th>\n      <th>total_phenols</th>\n      <th>flavanoids</th>\n      <th>nonflavanoid_phenols</th>\n      <th>proanthocyanins</th>\n      <th>color_intensity</th>\n      <th>hue</th>\n      <th>od280/od315_of_diluted_wines</th>\n      <th>proline</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.518613</td>\n      <td>-0.562250</td>\n      <td>0.232053</td>\n      <td>-1.169593</td>\n      <td>1.913905</td>\n      <td>0.808997</td>\n      <td>1.034819</td>\n      <td>-0.659563</td>\n      <td>1.224884</td>\n      <td>0.251717</td>\n      <td>0.362177</td>\n      <td>1.847920</td>\n      <td>1.013009</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.246290</td>\n      <td>-0.499413</td>\n      <td>-0.827996</td>\n      <td>-2.490847</td>\n      <td>0.018145</td>\n      <td>0.568648</td>\n      <td>0.733629</td>\n      <td>-0.820719</td>\n      <td>-0.544721</td>\n      <td>-0.293321</td>\n      <td>0.406051</td>\n      <td>1.113449</td>\n      <td>0.965242</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.196879</td>\n      <td>0.021231</td>\n      <td>1.109334</td>\n      <td>-0.268738</td>\n      <td>0.088358</td>\n      <td>0.808997</td>\n      <td>1.215533</td>\n      <td>-0.498407</td>\n      <td>2.135968</td>\n      <td>0.269020</td>\n      <td>0.318304</td>\n      <td>0.788587</td>\n      <td>1.395148</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.691550</td>\n      <td>-0.346811</td>\n      <td>0.487926</td>\n      <td>-0.809251</td>\n      <td>0.930918</td>\n      <td>2.491446</td>\n      <td>1.466525</td>\n      <td>-0.981875</td>\n      <td>1.032155</td>\n      <td>1.186068</td>\n      <td>-0.427544</td>\n      <td>1.184071</td>\n      <td>2.334574</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.295700</td>\n      <td>0.227694</td>\n      <td>1.840403</td>\n      <td>0.451946</td>\n      <td>1.281985</td>\n      <td>0.808997</td>\n      <td>0.663351</td>\n      <td>0.226796</td>\n      <td>0.401404</td>\n      <td>-0.319276</td>\n      <td>0.362177</td>\n      <td>0.449601</td>\n      <td>-0.037874</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do not delete this cell!\n",
    "data_standardized.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Elbow Method\n",
    "\n",
    "In cluster analysis, the elbow method is a heuristic used in determining the number of clusters in a data set. The elbow method plots the value of the cost function (inertia) produced by different values for the number of clusters. The elbow of the curve indicates the point that we should stop dividing the data into further clusters.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Task:` \n",
    "\n",
    "`In the function named elbow_method: - Implement the elbow method for Kmeans. - Plot the elbow`\n",
    "\n",
    "`Following that:- store in the variable called number_of_clusters the best number of clusters according to the elbow method for Kmeans`\n",
    "\n",
    "About the elbow plot:\n",
    "\n",
    "* x-axis: the number of clusters\n",
    "\n",
    "* y-axis: inertia/distortion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow_method(X, max_range_for_elbow, rseed = RSEED):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        X: dataframe or numpy array, the dataset to be clustered, \n",
    "        max_range_for_elbow: int, the max number of clusters you want the elbow method to run. \n",
    "\n",
    "    **1st way**:\n",
    "    step1: \n",
    "        create an empty list where you will store the inertia \n",
    "    step2:\n",
    "        Create a sequence of numbers from 1 to max_range_for_elbow and store in a variable called K, these will be your sequence of numbers for the various numbers of clusters\n",
    "    step3: \n",
    "        in a for loop that goes through the values of K one by one, run Kmeans for each of these values in the range with random_state=RSTATE\n",
    "    step4: \n",
    "        inside the for loop, calculate the inertia for the clustering that you currently ran and append in the list from step 1. \n",
    "        The inertia is an attribute of the Kmeans object\n",
    "    step5: \n",
    "        Outside the for loop, plot the resulting elbow, in x-axis: K (sequence of numbers from 1 to max_range_for_elbow+1) and in y-axis: the inertia for the respective values of K\n",
    "\n",
    "    **2nd way**:\n",
    "        Another way to implement the elbow method is to use the KElbowVisualizer from yellowbricks! \n",
    "        This link will help you get an insight on how you could use it: https://www.scikit-yb.org/en/latest/api/cluster/elbow.html\n",
    "    \n",
    "    **Note**\n",
    "        The above steps/different ways of implementing the elbow method are just indicative. Please implement this as you want as long as you plot the required elbow plot and\n",
    "        store in the variable called number_of_clusters the best number of clusters according to the elbow method for Kmeans.\n",
    "\n",
    "    \"\"\"\n",
    "    inertia_values = []\n",
    "    for k in range(1,max_range_for_elbow):\n",
    "        km = KMeans(n_clusters=k, random_state=rseed)\n",
    "        km = km.fit(X)\n",
    "        inertia_values.append(km.inertia_)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    plt.plot(range(1,max_range_for_elbow), inertia_values)\n",
    "    plt.xlabel('No. of Clusters', fontsize=15)\n",
    "    plt.ylabel('Inertia', fontsize=15)\n",
    "    plt.title('Inertia vs No. Of Clusters', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 576x432 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# do not delete this!\n",
    "# calling the elbow method here\n",
    "elbow_method(data_standardized,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the best number of clusters according to the elbow method for Kmeans? Find the elbow!\n",
    "# store the result here and use it for the rest of the assignment as input to the algorithms\n",
    "# do not delete this!\n",
    "\n",
    "number_of_clusters = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  5. Run K-means, K-medoids and Agglomerative Clustering on the wine dataset\n",
    "\n",
    "\n",
    "### `Task: Run K-means, K-medoids, Agglomerative with ward distance, Agglomerative with complete distance (use sklearn, scikit-learn-extra) and store the clustering labels in the variables indicated below for the respective algorithms. If you would like some help for this part you could either check the sklearn user-guide for the algorithms asked or check the examples of lab2.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run k-means (from sklearn) with the optimal number of clusters that you found above, and random_state=RSEED   \n",
    "# store the cluster memberships in kmeans_labels \n",
    "\n",
    "kmn = KMeans(n_clusters=number_of_clusters, random_state=RSEED)\n",
    "kmn.fit(X=data_standardized)\n",
    "kmeans_labels = kmn.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not delete this!\n",
    "labels_dict = {}\n",
    "\n",
    "labels_dict[\"kmeans\"] = kmeans_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run k-medoids (from scikit-learn-extra) with the optimal number of clusters that you found above, and random_state=RSEED   \n",
    "# store the cluster memberships in kmedoids_labels\n",
    "k_mediods = KMedoids(n_clusters=number_of_clusters, random_state=RSEED)\n",
    "k_mediods.fit(X=data_standardized)\n",
    "\n",
    "kmedoids_labels = k_mediods.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not delete this!\n",
    "\n",
    "labels_dict[\"kmedoids\"] = kmedoids_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run Agglomerative clustering (from sklearn) with ward distance and the optimal number of clusters that you found above\n",
    "# store the cluster memberships in ward_labels \n",
    "\n",
    "agglomerate_ward = AgglomerativeClustering(n_clusters=number_of_clusters,linkage=\"ward\")\n",
    "agglomerate_ward.fit(X=data_standardized)\n",
    "ward_labels = agglomerate_ward.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not delete this!\n",
    "labels_dict[\"ward\"] = ward_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run Agglomerative clustering (from sklearn) with complete distance and the optimal number of clusters that you found above\n",
    "# store the cluster memberships in complete_labels \n",
    "\n",
    "agglomerate_complete = AgglomerativeClustering(n_clusters=number_of_clusters, linkage=\"complete\")\n",
    "agglomerate_complete.fit(X=data_standardized)\n",
    "complete_labels  = agglomerate_complete.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not delete this!\n",
    "labels_dict[\"complete\"] = complete_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Task: Write a function that calculates the silhouette score (from sklearn) and the purity (use the function from task 2) for any given clustering.`\n",
    "\n",
    "Specifically:\n",
    "\n",
    "- calculate silhouette score for any given clustering with the given metric and rseed = RSEED in the function (use sklearn).\n",
    "- call the purity function that you defined above and calculate purity for any given clustering.\n",
    "- return the values for silhoutte score and purity in this order (s_s, pu).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_metrics(X, labels_pred, labels_true,  metric, rseed = RSEED):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        X: array-like, the dataframe used for clustering\n",
    "        labels_pred: numpy array, the labels predicted by the algorithm\n",
    "        labels_true: numpy array, the target/class\n",
    "        metric: string, the metric to be used for silhouette score, example: 'euclidean'          \n",
    "\n",
    "    Output:\n",
    "        s_s: the value as calculated by silhouette\n",
    "        pu: the value as calculated by the purity function\n",
    "\n",
    "    # Note: if you could not implement purity in task 2, return the s_s value as calculated and pu=0\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    s_s = metrics.silhouette_score(X=X, labels=labels_pred, metric=metric, random_state=rseed)\n",
    "    pu = purity(y_true=labels_true, y_pred=labels_pred)\n",
    "\n",
    "    return s_s, pu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering Algorithm:  kmeans\n",
      "Silhouette Score:  0.285 Purity:  0.966\n",
      "Clustering Algorithm:  kmedoids\n",
      "Silhouette Score:  0.266 Purity:  0.904\n",
      "Clustering Algorithm:  ward\n",
      "Silhouette Score:  0.277 Purity:  0.927\n",
      "Clustering Algorithm:  complete\n",
      "Silhouette Score:  0.204 Purity:  0.837\n"
     ]
    }
   ],
   "source": [
    "# run this to check your evaluation metrics for all the clusterings that you ran in task 5.  \n",
    "# do not delete this!\n",
    "\n",
    "\n",
    "for keys, values in labels_dict.items():\n",
    "    print(\"Clustering Algorithm: \", keys)\n",
    "    s_s, pu = evaluation_metrics(data_standardized, values, wine.target, 'euclidean', rseed = RSEED)\n",
    "    if pu == None:\n",
    "        print(\"Silhouette Score: \", np.round(s_s, decimals=3) , \"Purity: \", pu)\n",
    "    else:\n",
    "        print(\"Silhouette Score: \", np.round(s_s, decimals=3) , \"Purity: \", np.round(pu, decimals=3))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Plotting\n",
    "\n",
    "### `Task: Reduce the dimensionality of the dataset to 2 principal components. Create 1 figure with 5 plots where you will plot the 2 principal components with colors respective to the class (plot/column 1) and the clustering labels of the four algorithms (k-means, k-medoids, aggromerative with two different distances) that you ran in task 5 (plot/columns 2-5). `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "step 1: Reduce the dimensionality of the standardized dataset in 2 Principal Components, with Principal Component analysis.  store the reduced dataset in **pca_2d**\n",
    "step 2: Create a sublot object with 1 row and 5 columns\n",
    "step 3: Plot the two principal components (use a scatterplot) with colors respective to the true labels\n",
    "step 4-7: Plot the two principal components (use a scatterplot) with colors respective to the clustering labels of the algorithms that we ran above\n",
    "you can check Lab2-Unsuperised learning for a similar example\n",
    "don't forget to put the titles on the respective plots!\n",
    "\"\"\"\n",
    "\n",
    "pca_2d = np.zeros((10,10)) #change this\n",
    "\n",
    "# write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. K-medians "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Task: In the function called Kmedians implement Kmedians.`\n",
    "\n",
    "Detailed instructions can be found inside the method. \n",
    "\n",
    "**K-medians:** Instead of taking the mean value of the object in a cluster as a reference point, medians are used (**L1-norm** as the distance measure)\n",
    "\n",
    "\n",
    "The K-medians clustering algorithm:\n",
    "\n",
    "\n",
    "-Select K points as the initial representative objects (choose the initial random points from the dataset samples)\n",
    "\n",
    "-Repeat\n",
    "\n",
    "* Assign every point to its nearest **median**\n",
    "\n",
    "* Re-compute the median of each individual feature\n",
    "    \n",
    "Until convergence is satisfied   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kmedians(X, n_clusters, rseed=RSEED):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        X: numpy array, the dataset to be clustered, \n",
    "        n_clusters: the number of clusters you want your algorithm to run with, \n",
    "        RSEED: the random seed\n",
    "    Output:\n",
    "        centers: numpy array, the cluster centers of the algorithm you are currently running, \n",
    "        labels: numpy array, the clustering labels\n",
    "\n",
    "    step 1: Generate a random number using np.random.RandomState and with the random seed provided in parameters\n",
    "    step 2: Get the n_clusters first elements from a randomly permulated sequence with the length equal to the number of rows in X using the random number from step 1\n",
    "    step 3: Retrieve from X the elements with the indices found above and make those as our first random centers\n",
    "    step 4: assign each point to the nearest centers using the L1-norm as the distance measure (manhattan) and store it in a variable called labels \n",
    "    step 5: Update the centroids of each cluster using the median and store in a variable called new_centers\n",
    "    step 6: while the new_centers (from step 5) are not equal to the previously created centers, repeat the process in step 4 and 5.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # write your code here \n",
    "\n",
    "\n",
    "    return np.zeros((4)), np.zeros(10) # change this zero np arrays to the proper outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [178, 10]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-165-dab224d8958d>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mcenters\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mKmedians\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata_standardized\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnumber_of_clusters\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0ms_s\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpu\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mevaluation_metrics\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata_standardized\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mwine\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtarget\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'euclidean'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrseed\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mRSEED\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-161-da059b06e0b0>\u001B[0m in \u001B[0;36mevaluation_metrics\u001B[0;34m(X, labels_pred, labels_true, metric, rseed)\u001B[0m\n\u001B[1;32m     15\u001B[0m     \"\"\"\n\u001B[1;32m     16\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 17\u001B[0;31m     \u001B[0ms_s\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmetrics\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msilhouette_score\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlabels_pred\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmetric\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmetric\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrandom_state\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mrseed\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     18\u001B[0m     \u001B[0mpu\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpurity\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my_true\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlabels_true\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_pred\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlabels_pred\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/data-mining/lib/python3.6/site-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36minner_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     61\u001B[0m             \u001B[0mextra_args\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mall_args\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mextra_args\u001B[0m \u001B[0;34m<=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 63\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     64\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     65\u001B[0m             \u001B[0;31m# extra_args > 0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/data-mining/lib/python3.6/site-packages/sklearn/metrics/cluster/_unsupervised.py\u001B[0m in \u001B[0;36msilhouette_score\u001B[0;34m(X, labels, metric, sample_size, random_state, **kwds)\u001B[0m\n\u001B[1;32m    115\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    116\u001B[0m             \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mindices\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mindices\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 117\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msilhouette_samples\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmetric\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmetric\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    118\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/data-mining/lib/python3.6/site-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36minner_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     61\u001B[0m             \u001B[0mextra_args\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mall_args\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mextra_args\u001B[0m \u001B[0;34m<=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 63\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     64\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     65\u001B[0m             \u001B[0;31m# extra_args > 0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/data-mining/lib/python3.6/site-packages/sklearn/metrics/cluster/_unsupervised.py\u001B[0m in \u001B[0;36msilhouette_samples\u001B[0;34m(X, labels, metric, **kwds)\u001B[0m\n\u001B[1;32m    212\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    213\u001B[0m     \"\"\"\n\u001B[0;32m--> 214\u001B[0;31m     \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcheck_X_y\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maccept_sparse\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'csc'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'csr'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    215\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    216\u001B[0m     \u001B[0;31m# Check for non-zero diagonal entries in precomputed distance matrix\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/data-mining/lib/python3.6/site-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36minner_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     61\u001B[0m             \u001B[0mextra_args\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mall_args\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mextra_args\u001B[0m \u001B[0;34m<=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 63\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     64\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     65\u001B[0m             \u001B[0;31m# extra_args > 0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/data-mining/lib/python3.6/site-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36mcheck_X_y\u001B[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001B[0m\n\u001B[1;32m    886\u001B[0m         \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloat64\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    887\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 888\u001B[0;31m     \u001B[0mcheck_consistent_length\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    889\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    890\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/data-mining/lib/python3.6/site-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36mcheck_consistent_length\u001B[0;34m(*arrays)\u001B[0m\n\u001B[1;32m    318\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0muniques\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    319\u001B[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001B[0;32m--> 320\u001B[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001B[0m\u001B[1;32m    321\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    322\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: Found input variables with inconsistent numbers of samples: [178, 10]"
     ]
    }
   ],
   "source": [
    "# do not delete this!\n",
    "\n",
    "centers, labels = Kmedians(data_standardized.values, number_of_clusters)\n",
    "\n",
    "s_s, pu = evaluation_metrics(data_standardized.values, labels, wine.target, 'euclidean', rseed = RSEED)\n",
    "\n",
    "\n",
    "sns.scatterplot(x=pca_2d[:, 0],y=pca_2d[:, 1], hue=labels)\n",
    "if pu == None:\n",
    "    plt.title(\"Wine Kmedians, PU: {},  SS: {}\".format(pu, np.round(s_s, decimals=3)))\n",
    "else:\n",
    "    plt.title(\"Wine Kmedians, PU: {},  SS: {}\".format(np.round(pu, decimals=3), np.round(s_s, decimals=3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `NOTE: Each function you make will be graded, so it is important to strictly follow input and output instructions stated in the skeleton code. Some of the cells have already some variables that are filled with None values or empty dataframes, you should change those nan/empty values (we refer to it as 'change this') to what is asked in the tasks (we only stored the empty values so the whole notebook can run error free). You should not delete any of the given cells as they will help us grade the assignment. Some cells ask you to uncomment some comments, please only do so if you have solved the respective task. When you are finished with implementing all the tasks, clear all outputs,  **restart the kernel**, run all cells again (make sure there is no error) and submit! Make sure that the results and figures asked are visible for us to grade. ` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF ASSIGNMENT 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d152863b18335c47a24d1450da55dce515e7f4874b41ba8bc3d23bdf8dfe1916"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}