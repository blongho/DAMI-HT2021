{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 3: Classification, Evaluation, and Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, you will experience a complete machine learning cycle from data preparation to deployment. You will prepare a dataset, make a model, evaluate models to find the best fit, and deploy it to a simple web page. Our main objective is to make you try classification and evaluation methods, so we will only apply essential data preprocessing techniques but mainly focus on classification and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the **Adult** dataset from the UCI repository and more information about the data is available [here](http://archive.ics.uci.edu/ml/datasets/Adult). Since we have removed and changed the dataset for a grading purpose, use the one that we provide on ilearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains the information to check whether income exceeds $50K/yr based on census data. The datasets consist of 14 attributes and one binary class variable:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- income: >50K, <=50K\n",
    "\n",
    "- age: continuous.\n",
    "\n",
    "- workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n",
    "\n",
    "- fnlwgt: continuous.\n",
    "\n",
    "- education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool. education-num: continuous.\n",
    "\n",
    "- education-num: continuous.\n",
    "\n",
    "- marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n",
    "\n",
    "- occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n",
    "\n",
    "- relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n",
    "\n",
    "- race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n",
    "\n",
    "- sex: Female, Male.\n",
    "\n",
    "- capital-gain: continuous.\n",
    "\n",
    "- capital-loss: continuous.\n",
    "\n",
    "- hours-per-week: continuous.\n",
    "\n",
    "- native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we have a binary class which can be `>=50` or `<50`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**\n",
    "- Unlike the labs, each function you make here will be **graded**, so it is important to *strictly* follow the instruction.\n",
    "- **Import** all necessary libraries yourself whenever needed. Failure to run any code can affect your grade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Total points: 6.0 pt**\n",
    "\n",
    "0. Preparation (0.7 pt)\n",
    " - Task 1: Drop missing values (0.1 pt).\n",
    " - Task 2: Assign X and y (0.1 pt).\n",
    " - Task 3: One-hot encoding (0.1 pt).\n",
    " - Task 4: Train test split (0.2 pt).\n",
    " - Task 5: Standardization (0.2 pt).\n",
    "1. Classification (2.9 pt)\n",
    " - Task 6: Random forest (0.5 pt).\n",
    " - Task 7: SVM with diverse kernels (0.4 pt).\n",
    " - Task 8: Decision tree and Random Forest (2.0 pt).\n",
    "2. Evaluation (1.8 pt)\n",
    " - Task 9: Accuracy, Precision, Recall, F1-score (0.3 pt).\n",
    " - Task 10: AUC/AUPRC (0.3 pt).\n",
    " - Task 11: Apply them together with scikit-learn (0.4 pt).\n",
    " - Task 12: Manual implementation of performance metrics (0.8 pt).\n",
    "3. Deployment (0.6 pt)\n",
    " - Task 13: Save models into a file using pickle (0.3 pt).\n",
    " - Task 14: DASH deployment (0.3 pt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Student information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please provide your information for automatic grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "STUD_SUID = 'lobe2042'\n",
    "STUD_NAME = 'Longho Bernard Che'\n",
    "STUD_EMAIL = 'lobe2042@student.su.se'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These libraries will be frequently used throughout the homework. Do not change the block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "RANDOM_STATE = 12345  #Do not change it!\n",
    "np.random.seed(RANDOM_STATE)  #Do not change it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the **Adult** dataset located ilearn, and load it here using pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult = pd.read_csv(\"datasets/adult.data\", sep=\",\", header=None, skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the line below to give the dataframe proper column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult.columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation',\n",
    "                 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',\n",
    "                 'income']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can find out some basic information by calling *info(), head()*, and *describe()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32561 entries, 0 to 32560\n",
      "Data columns (total 15 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   age             32561 non-null  int64 \n",
      " 1   workclass       32561 non-null  object\n",
      " 2   fnlwgt          32561 non-null  int64 \n",
      " 3   education       32561 non-null  object\n",
      " 4   education-num   32561 non-null  int64 \n",
      " 5   marital-status  32561 non-null  object\n",
      " 6   occupation      32561 non-null  object\n",
      " 7   relationship    32561 non-null  object\n",
      " 8   race            32561 non-null  object\n",
      " 9   sex             32561 non-null  object\n",
      " 10  capital-gain    32561 non-null  int64 \n",
      " 11  capital-loss    32561 non-null  int64 \n",
      " 12  hours-per-week  32561 non-null  int64 \n",
      " 13  native-country  32561 non-null  object\n",
      " 14  income          32561 non-null  object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "adult.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  fnlwgt  education  education-num  \\\n",
       "0   39         State-gov   77516  Bachelors             13   \n",
       "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
       "2   38           Private  215646    HS-grad              9   \n",
       "3   53           Private  234721       11th              7   \n",
       "4   28           Private  338409  Bachelors             13   \n",
       "\n",
       "       marital-status         occupation   relationship   race     sex  \\\n",
       "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
       "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
       "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
       "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
       "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week native-country income  \n",
       "0          2174             0              40  United-States  <=50K  \n",
       "1             0             0              13  United-States  <=50K  \n",
       "2             0             0              40  United-States  <=50K  \n",
       "3             0             0              40  United-States  <=50K  \n",
       "4             0             0              40           Cuba  <=50K  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>32561.000000</td>\n",
       "      <td>3.256100e+04</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.581647</td>\n",
       "      <td>1.897784e+05</td>\n",
       "      <td>10.080679</td>\n",
       "      <td>1077.648844</td>\n",
       "      <td>87.303830</td>\n",
       "      <td>40.437456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.640433</td>\n",
       "      <td>1.055500e+05</td>\n",
       "      <td>2.572720</td>\n",
       "      <td>7385.292085</td>\n",
       "      <td>402.960219</td>\n",
       "      <td>12.347429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.228500e+04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>1.178270e+05</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>1.783560e+05</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>2.370510e+05</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>1.484705e+06</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>4356.000000</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age        fnlwgt  education-num  capital-gain  capital-loss  \\\n",
       "count  32561.000000  3.256100e+04   32561.000000  32561.000000  32561.000000   \n",
       "mean      38.581647  1.897784e+05      10.080679   1077.648844     87.303830   \n",
       "std       13.640433  1.055500e+05       2.572720   7385.292085    402.960219   \n",
       "min       17.000000  1.228500e+04       1.000000      0.000000      0.000000   \n",
       "25%       28.000000  1.178270e+05       9.000000      0.000000      0.000000   \n",
       "50%       37.000000  1.783560e+05      10.000000      0.000000      0.000000   \n",
       "75%       48.000000  2.370510e+05      12.000000      0.000000      0.000000   \n",
       "max       90.000000  1.484705e+06      16.000000  99999.000000   4356.000000   \n",
       "\n",
       "       hours-per-week  \n",
       "count    32561.000000  \n",
       "mean        40.437456  \n",
       "std         12.347429  \n",
       "min          1.000000  \n",
       "25%         40.000000  \n",
       "50%         40.000000  \n",
       "75%         45.000000  \n",
       "max         99.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like there is no null data. However, if you read the description of the dataset, it says that there are missing parts represented as \"?\". You can count them by using the same technique we used for checking nulls in the previous lab. We have missing values in specific columns only, and it is about 5% of data records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                  0\n",
       "workclass         1836\n",
       "fnlwgt               0\n",
       "education            0\n",
       "education-num        0\n",
       "marital-status       0\n",
       "occupation        1843\n",
       "relationship         0\n",
       "race                 0\n",
       "sex                  0\n",
       "capital-gain         0\n",
       "capital-loss         0\n",
       "hours-per-week       0\n",
       "native-country     583\n",
       "income               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(adult == \"?\").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Drop missing values (0.1 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to handle missing values, such as imputation or putting median/mean values, but we will practice the simplest way: removing the rows with missing values.\n",
    "\n",
    "- Complete the function below which removes any rows with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_missing_values(df, miss):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "      df: the dataframe (adult in our case)\n",
    "      miss: a character to represent missing value (\"?\" in our case)\n",
    "      \n",
    "    Output: the dataframe without the missing values\n",
    "\n",
    "    Step 1: Replace the value 'miss' with np.nan.\n",
    "    Step 2: Drop the nan values and store the result in data_dropped.\n",
    "    Step 3: Return data_dropped\n",
    "    \n",
    "    \"\"\"\n",
    "    df.replace(to_replace=miss, value=np.nan, inplace=True)\n",
    "    data_dropped = df.dropna(axis=0)  # drop any row with np.nan\n",
    "    return data_dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply your function to our dataset `adult` and save the result to `adult_dropped`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_dropped = drop_missing_values(adult, \"?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the function should have the same attributes but only less number of the rows. Check how many rows are removed. Your dataset should have 30,162 rows!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30162, 15)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult_dropped.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Assign X and y (0.1 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's split our dataset into two parts (`X` for attributes and `y` for labels) to use scikit-learn's various methods.\n",
    "- Use `adult_dropped`.\n",
    "- `X` should have all the attributes without the labels (the last column).\n",
    "- `y` should be a Pandas Series only with the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(adult_dropped, columns=adult_dropped.columns[:-1])  # CHANGE IT!\n",
    "y = pd.Series(adult_dropped.iloc[:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the type and size here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30162, 14), (30162,), pandas.core.frame.DataFrame, pandas.core.series.Series)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X.shape, y.shape, type(X), type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: One-hot encoding (0.1 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, scikit-learn does not support categorical attributes very well even for decision tree, and that means we need to convert them into reasonal form of numerical data to fit the algorithms. There is one way called one-hot encoding, which transforms the categorical data into multiple numerical columns for each possible value. There are various ways to apply this, especially using [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) or [pandas](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html) but here we will use the pandas function to keep the dataframe structure.\n",
    "\n",
    "- Finish one_hot_encoding function which applies one-hot encoding to a given dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def one_hot_encoding(df):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        df: the attributes (X in our case)\n",
    "    Output: one-hot encoded dataframe\n",
    "    \n",
    "    Step 1: Use pd.get_dummies to convert df to a one-hot-encoded form. \n",
    "            Enable an option called drop_first to remove duplication.\n",
    "    Step 2: Return the one-hot-encoded dataframe.\n",
    "    \n",
    "    * Those steps and suggested method are just for your convenience. You can use your own choice of methods.\n",
    "    \"\"\"\n",
    "    df_onehot = pd.get_dummies(data=df, drop_first=True)\n",
    "    return df_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create `X_onehot` by calling `one_hot_encoding` function with `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_onehot = one_hot_encoding(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your result by calling any methods you learned. If you successfully followed the instruction, the output (`X_onehot`) should have 96 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>workclass_Local-gov</th>\n",
       "      <th>workclass_Private</th>\n",
       "      <th>workclass_Self-emp-inc</th>\n",
       "      <th>workclass_Self-emp-not-inc</th>\n",
       "      <th>...</th>\n",
       "      <th>native-country_Portugal</th>\n",
       "      <th>native-country_Puerto-Rico</th>\n",
       "      <th>native-country_Scotland</th>\n",
       "      <th>native-country_South</th>\n",
       "      <th>native-country_Taiwan</th>\n",
       "      <th>native-country_Thailand</th>\n",
       "      <th>native-country_Trinadad&amp;Tobago</th>\n",
       "      <th>native-country_United-States</th>\n",
       "      <th>native-country_Vietnam</th>\n",
       "      <th>native-country_Yugoslavia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>77516</td>\n",
       "      <td>13</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>83311</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>215646</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>234721</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>338409</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  fnlwgt  education-num  capital-gain  capital-loss  hours-per-week  \\\n",
       "0   39   77516             13          2174             0              40   \n",
       "1   50   83311             13             0             0              13   \n",
       "2   38  215646              9             0             0              40   \n",
       "3   53  234721              7             0             0              40   \n",
       "4   28  338409             13             0             0              40   \n",
       "\n",
       "   workclass_Local-gov  workclass_Private  workclass_Self-emp-inc  \\\n",
       "0                    0                  0                       0   \n",
       "1                    0                  0                       0   \n",
       "2                    0                  1                       0   \n",
       "3                    0                  1                       0   \n",
       "4                    0                  1                       0   \n",
       "\n",
       "   workclass_Self-emp-not-inc  ...  native-country_Portugal  \\\n",
       "0                           0  ...                        0   \n",
       "1                           1  ...                        0   \n",
       "2                           0  ...                        0   \n",
       "3                           0  ...                        0   \n",
       "4                           0  ...                        0   \n",
       "\n",
       "   native-country_Puerto-Rico  native-country_Scotland  native-country_South  \\\n",
       "0                           0                        0                     0   \n",
       "1                           0                        0                     0   \n",
       "2                           0                        0                     0   \n",
       "3                           0                        0                     0   \n",
       "4                           0                        0                     0   \n",
       "\n",
       "   native-country_Taiwan  native-country_Thailand  \\\n",
       "0                      0                        0   \n",
       "1                      0                        0   \n",
       "2                      0                        0   \n",
       "3                      0                        0   \n",
       "4                      0                        0   \n",
       "\n",
       "   native-country_Trinadad&Tobago  native-country_United-States  \\\n",
       "0                               0                             1   \n",
       "1                               0                             1   \n",
       "2                               0                             1   \n",
       "3                               0                             1   \n",
       "4                               0                             0   \n",
       "\n",
       "   native-country_Vietnam  native-country_Yugoslavia  \n",
       "0                       0                          0  \n",
       "1                       0                          0  \n",
       "2                       0                          0  \n",
       "3                       0                          0  \n",
       "4                       0                          0  \n",
       "\n",
       "[5 rows x 96 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_onehot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4: Train test split (0.2 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to split our dataset further into four parts for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use scikit-learn's `train_test_split` function to divide the dataset into four parts.\n",
    "- Follow the instruction below carefully to get a point!.\n",
    "    - Use `X_onehot` and `y`.\n",
    "    - Assign 30% to a test set.\n",
    "    - Use our random state (`RANDOM_STATE`)\n",
    "    - Enable stratify option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Remove the assigned values and write train_test_split function\n",
    "X_train, X_test, y_train, y_test = train_test_split(one_hot_encoding(X), y, test_size=30, random_state=RANDOM_STATE,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the type and size here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30132, 96), (30, 96), (30132,), (30,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5: Standardization (0.2 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing the missing value and split X and y, we need to take care of our numerical attributes. As you can check from `describe()` function, we have five numerical attributes and they all have different mean and standard deviation. Not all machine learning models require standardization of numerical attributes, but some do. In this homework, SVM might be the case that the standardization is required. It might be better to make standardized version when performing data preparation. \n",
    "\n",
    "- One-hot encoded data does not need to be standardized! So you need to choose the numerical columns only\n",
    " - [\"age\", \"fnlwgt\",  \"education-num\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]\n",
    " \n",
    "- For this task, you need to import sklearn's `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERICAL_ATTRIBUTES = [\"age\", \"fnlwgt\", \"education-num\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from pandas import DataFrame\n",
    "\n",
    "\n",
    "def standardize(X_train, X_test, numerical=NUMERICAL_ATTRIBUTES):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - X_train: A split training set from Task 4\n",
    "        - X_test: A split test set from Task 5\n",
    "        - numerical: Numerical columns that should be standardized\n",
    "    Output:\n",
    "        - X_train_st: A standardized numerical attributes (ndarray)\n",
    "        - X_test_st: A standardized numerical attributes (ndarray)\n",
    "    \n",
    "    Step 1: Initialize StandardScaler.\n",
    "    Step 2: Create X_train_numerical, X_test_numerical by selecting numerical columns from original X_train X_test.\n",
    "    Step 3: Fit StandardScaler on X_train. You should only use the numerical columns only.\n",
    "    Step 4: Use trained StandardScaler and run transform function both on X_train_st (for the training set) \n",
    "            and X_test_st (for the test set). This job will standardize both training and test sets based on\n",
    "            the statistics of training set. You should only use numerical attributes.\n",
    "    Step 5: Return X_train_st, X_test_st\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1\n",
    "    sc = StandardScaler()\n",
    "\n",
    "    # Step 2\n",
    "    X_train_numeric = DataFrame(data=X_train, columns=numerical)\n",
    "    X_test_numeric = DataFrame(data=X_test, columns=numerical)\n",
    "\n",
    "    # Step 3\n",
    "\n",
    "    # Step 4\n",
    "    # Assign two outputs of transformation function to X_train_st (for the training set) and X_test_st (for the test set)\n",
    "    X_train_st = sc.fit_transform(X_train_numeric)\n",
    "    X_test_st = sc.fit_transform(X_test_numeric)\n",
    "\n",
    "    # Step 5\n",
    "    # Note that those two variable should only contain numerical attributes, not the whole ones.\n",
    "    return X_train_st, X_test_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_wrapper(X_train, X_test):\n",
    "    # DO NOT CHANGE THIS FUNCTION\n",
    "    # This function is to ensure that the datasets keep the Pandas DataFrame format.\n",
    "    if X_train.shape == (0, 0): return pd.DataFrame([0]), pd.DataFrame([0])\n",
    "\n",
    "    numerical = [\"age\", \"fnlwgt\", \"education-num\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]\n",
    "\n",
    "    X_train_st, X_test_st = standardize(X_train, X_test)\n",
    "\n",
    "    X_train_st_df = X_train.copy()\n",
    "    X_train_st_df[numerical] = X_train_st\n",
    "    X_test_st_df = X_test.copy()\n",
    "    X_test_st_df[numerical] = X_test_st\n",
    "\n",
    "    return X_train_st_df, X_test_st_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line below will apply your standardization function to the datasets. Run the block and check the result. \n",
    "DO NOT change `standardize_wrapper`, just implement `standardize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_st, X_test_st = standardize_wrapper(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your numerical attributes ([\"age\", \"fnlwgt\", \"education-num\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]) should have near zero mean and one standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>workclass_Local-gov</th>\n",
       "      <th>workclass_Private</th>\n",
       "      <th>workclass_Self-emp-inc</th>\n",
       "      <th>workclass_Self-emp-not-inc</th>\n",
       "      <th>...</th>\n",
       "      <th>native-country_Portugal</th>\n",
       "      <th>native-country_Puerto-Rico</th>\n",
       "      <th>native-country_Scotland</th>\n",
       "      <th>native-country_South</th>\n",
       "      <th>native-country_Taiwan</th>\n",
       "      <th>native-country_Thailand</th>\n",
       "      <th>native-country_Trinadad&amp;Tobago</th>\n",
       "      <th>native-country_United-States</th>\n",
       "      <th>native-country_Vietnam</th>\n",
       "      <th>native-country_Yugoslavia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.013200e+04</td>\n",
       "      <td>3.013200e+04</td>\n",
       "      <td>3.013200e+04</td>\n",
       "      <td>3.013200e+04</td>\n",
       "      <td>3.013200e+04</td>\n",
       "      <td>3.013200e+04</td>\n",
       "      <td>30132.000000</td>\n",
       "      <td>30132.000000</td>\n",
       "      <td>30132.000000</td>\n",
       "      <td>30132.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>30132.000000</td>\n",
       "      <td>30132.000000</td>\n",
       "      <td>30132.000000</td>\n",
       "      <td>30132.000000</td>\n",
       "      <td>30132.000000</td>\n",
       "      <td>30132.000000</td>\n",
       "      <td>30132.000000</td>\n",
       "      <td>30132.000000</td>\n",
       "      <td>30132.000000</td>\n",
       "      <td>30132.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-8.960781e-17</td>\n",
       "      <td>8.689599e-17</td>\n",
       "      <td>-1.273374e-16</td>\n",
       "      <td>-1.768575e-17</td>\n",
       "      <td>-8.394837e-17</td>\n",
       "      <td>9.715373e-17</td>\n",
       "      <td>0.068565</td>\n",
       "      <td>0.738816</td>\n",
       "      <td>0.035643</td>\n",
       "      <td>0.082869</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001128</td>\n",
       "      <td>0.003617</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>0.002356</td>\n",
       "      <td>0.001394</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>0.000597</td>\n",
       "      <td>0.911987</td>\n",
       "      <td>0.002124</td>\n",
       "      <td>0.000531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>0.252717</td>\n",
       "      <td>0.439287</td>\n",
       "      <td>0.185402</td>\n",
       "      <td>0.275688</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033573</td>\n",
       "      <td>0.060037</td>\n",
       "      <td>0.019103</td>\n",
       "      <td>0.048485</td>\n",
       "      <td>0.037309</td>\n",
       "      <td>0.023746</td>\n",
       "      <td>0.024434</td>\n",
       "      <td>0.283318</td>\n",
       "      <td>0.046039</td>\n",
       "      <td>0.023038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.632097e+00</td>\n",
       "      <td>-1.665894e+00</td>\n",
       "      <td>-3.578372e+00</td>\n",
       "      <td>-1.474683e-01</td>\n",
       "      <td>-2.185385e-01</td>\n",
       "      <td>-3.333019e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-7.945625e-01</td>\n",
       "      <td>-6.830221e-01</td>\n",
       "      <td>-4.403189e-01</td>\n",
       "      <td>-1.474683e-01</td>\n",
       "      <td>-2.185385e-01</td>\n",
       "      <td>-7.783481e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-1.093071e-01</td>\n",
       "      <td>-1.075752e-01</td>\n",
       "      <td>-4.806225e-02</td>\n",
       "      <td>-1.474683e-01</td>\n",
       "      <td>-2.185385e-01</td>\n",
       "      <td>-7.783481e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.520878e-01</td>\n",
       "      <td>4.527381e-01</td>\n",
       "      <td>1.128708e+00</td>\n",
       "      <td>-1.474683e-01</td>\n",
       "      <td>-2.185385e-01</td>\n",
       "      <td>3.394965e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.926086e+00</td>\n",
       "      <td>1.225568e+01</td>\n",
       "      <td>2.305478e+00</td>\n",
       "      <td>1.334802e+01</td>\n",
       "      <td>1.055593e+01</td>\n",
       "      <td>4.846675e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                age        fnlwgt  education-num  capital-gain  capital-loss  \\\n",
       "count  3.013200e+04  3.013200e+04   3.013200e+04  3.013200e+04  3.013200e+04   \n",
       "mean  -8.960781e-17  8.689599e-17  -1.273374e-16 -1.768575e-17 -8.394837e-17   \n",
       "std    1.000017e+00  1.000017e+00   1.000017e+00  1.000017e+00  1.000017e+00   \n",
       "min   -1.632097e+00 -1.665894e+00  -3.578372e+00 -1.474683e-01 -2.185385e-01   \n",
       "25%   -7.945625e-01 -6.830221e-01  -4.403189e-01 -1.474683e-01 -2.185385e-01   \n",
       "50%   -1.093071e-01 -1.075752e-01  -4.806225e-02 -1.474683e-01 -2.185385e-01   \n",
       "75%    6.520878e-01  4.527381e-01   1.128708e+00 -1.474683e-01 -2.185385e-01   \n",
       "max    3.926086e+00  1.225568e+01   2.305478e+00  1.334802e+01  1.055593e+01   \n",
       "\n",
       "       hours-per-week  workclass_Local-gov  workclass_Private  \\\n",
       "count    3.013200e+04         30132.000000       30132.000000   \n",
       "mean     9.715373e-17             0.068565           0.738816   \n",
       "std      1.000017e+00             0.252717           0.439287   \n",
       "min     -3.333019e+00             0.000000           0.000000   \n",
       "25%     -7.783481e-02             0.000000           0.000000   \n",
       "50%     -7.783481e-02             0.000000           1.000000   \n",
       "75%      3.394965e-01             0.000000           1.000000   \n",
       "max      4.846675e+00             1.000000           1.000000   \n",
       "\n",
       "       workclass_Self-emp-inc  workclass_Self-emp-not-inc  ...  \\\n",
       "count            30132.000000                30132.000000  ...   \n",
       "mean                 0.035643                    0.082869  ...   \n",
       "std                  0.185402                    0.275688  ...   \n",
       "min                  0.000000                    0.000000  ...   \n",
       "25%                  0.000000                    0.000000  ...   \n",
       "50%                  0.000000                    0.000000  ...   \n",
       "75%                  0.000000                    0.000000  ...   \n",
       "max                  1.000000                    1.000000  ...   \n",
       "\n",
       "       native-country_Portugal  native-country_Puerto-Rico  \\\n",
       "count             30132.000000                30132.000000   \n",
       "mean                  0.001128                    0.003617   \n",
       "std                   0.033573                    0.060037   \n",
       "min                   0.000000                    0.000000   \n",
       "25%                   0.000000                    0.000000   \n",
       "50%                   0.000000                    0.000000   \n",
       "75%                   0.000000                    0.000000   \n",
       "max                   1.000000                    1.000000   \n",
       "\n",
       "       native-country_Scotland  native-country_South  native-country_Taiwan  \\\n",
       "count             30132.000000          30132.000000           30132.000000   \n",
       "mean                  0.000365              0.002356               0.001394   \n",
       "std                   0.019103              0.048485               0.037309   \n",
       "min                   0.000000              0.000000               0.000000   \n",
       "25%                   0.000000              0.000000               0.000000   \n",
       "50%                   0.000000              0.000000               0.000000   \n",
       "75%                   0.000000              0.000000               0.000000   \n",
       "max                   1.000000              1.000000               1.000000   \n",
       "\n",
       "       native-country_Thailand  native-country_Trinadad&Tobago  \\\n",
       "count             30132.000000                    30132.000000   \n",
       "mean                  0.000564                        0.000597   \n",
       "std                   0.023746                        0.024434   \n",
       "min                   0.000000                        0.000000   \n",
       "25%                   0.000000                        0.000000   \n",
       "50%                   0.000000                        0.000000   \n",
       "75%                   0.000000                        0.000000   \n",
       "max                   1.000000                        1.000000   \n",
       "\n",
       "       native-country_United-States  native-country_Vietnam  \\\n",
       "count                  30132.000000            30132.000000   \n",
       "mean                       0.911987                0.002124   \n",
       "std                        0.283318                0.046039   \n",
       "min                        0.000000                0.000000   \n",
       "25%                        1.000000                0.000000   \n",
       "50%                        1.000000                0.000000   \n",
       "75%                        1.000000                0.000000   \n",
       "max                        1.000000                1.000000   \n",
       "\n",
       "       native-country_Yugoslavia  \n",
       "count               30132.000000  \n",
       "mean                    0.000531  \n",
       "std                     0.023038  \n",
       "min                     0.000000  \n",
       "25%                     0.000000  \n",
       "50%                     0.000000  \n",
       "75%                     0.000000  \n",
       "max                     1.000000  \n",
       "\n",
       "[8 rows x 96 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_st.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finishing a simple data processing, let's proceed to our main task, classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will run random forest (RF), and support vector machine (SVM) with different kernels using scikit-learn. Then we will implement score functions for decision trees and main functions for random forests to understand the concepts better. We will continue to use the pre-processed Adult dataset from the section zero (Task 1-5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6: Random forest (graded, 0.5 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you will run the random forest algorithm using scikit-learn, together with cross-validation. Detailed information about the random forest in scikit-learn can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\n",
    "\n",
    "Your task is as follows:\n",
    " 1. Create a random forest classifier `rf` with the random state defined above (`RANDOM_STATE`). Do not specify any other parameters.\n",
    " 2. Report an average cross-validation score `rf_cross_val_score` with stratified k-fold with **cv=5**. You should report the average score, not a list of the scores. Use `X_onehot` and `y`, not the training or test set (0.2 pt). \n",
    " - ***Note that you are reporting an average cross validation score, not a list of scores.***\n",
    "\n",
    " 3. Run grid search `gs` with a single dictionary `grid_dict` with two keys 1) max_depth from 1 to 3 (included), and 2) min_samples_split from 2 to 4 (included) and report the best classifier into the variable `rf_best_classifier`. Set **cv=5** for grid search cross-validation. Since grid search uses stratified k-fold inside, you should put a complete dataset, not a split training set (Use `X_onehot` and `y`). This task can take more than five minutes depending on computing power (0.3 pt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "\n",
    "grid_dict = {\n",
    "    'max_depth': [1, 2, 3],\n",
    "    'min_samples_split': [2, 3, 4]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "\n",
    "# train the model\n",
    "rf.fit(X=X_onehot, y=y)\n",
    "\n",
    "# predict the labels\n",
    "predictions = rf.predict(X=X_onehot)\n",
    "#print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_cross_val_score_list = cross_val_score(estimator=rf, X=X_onehot, y=y, cv=5)\n",
    "rf_cross_val_score = np.mean(rf_cross_val_score_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this line to check your score. Your score should be above 0.80."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8500763484302297"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(random_state=12345),\n",
       "             param_grid={'max_depth': [1, 2, 3],\n",
       "                         'min_samples_split': [2, 3, 4]})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = GridSearchCV(estimator=rf, param_grid=grid_dict, cv=5)\n",
    "gs.fit(X_onehot, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best_classifier = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report your best classifier here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=3, random_state=12345)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_best_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 7: SVM with diverse kernels (graded, 0.4 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already tried a simple SVC with the RBF kernel before. Here you will run SVM again, but with different kernels, and together with cross-validation. Detailed information about SVC in scikit-learn can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC).\n",
    "\n",
    "Your task is as follows:\n",
    "\n",
    "  1. Create a standard SVC classifier `svc` without setting any parameter.\n",
    "  2. Report a test score to `svm_ho_score` using your standardized training set `X_train_st`. and test set `X_test_st` (0.1 pt).\n",
    "  3. Run grid search with a list of two parameter dictionaries, one with kernel = ['linear', 'poly', 'rbf'] and the other one with C = [1, 10, 100]. This means you have to create a list containing two different dictionaries. Report the best classifier into the variable `svm_best_classifier`. Set **cv=3** for grid search cross-validation. Use `X_train_st` and `X_test_st`. This task can take more than five minutes depending on computing power (0.3 pt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC()\n",
    "svc.fit(X=X_train_st, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_ho_score = svc.score(X=X_test_st, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this line to check your score. Your score should be above 0.80."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7666666666666667"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_ho_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "grid_dict_ = [\n",
    "    {'kernel': ['linear', 'poly', 'rbf']},\n",
    "    {'C': [1, 10, 100]}\n",
    "]\n",
    "\n",
    "gs = GridSearchCV(estimator=svc, param_grid=grid_dict_, cv=5)\n",
    "gs.fit(X=X_train_st, y=y_train)\n",
    "svm_best_classifier = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "svm_best_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 8: Decision tree and Random Forest (2.0 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now implement a few modules for decision tree and random forest. Follow the instruction carefully so that you can return a correct result. This task is composed of two subtasks as follows:\n",
    "\n",
    "  - 8-1. Entropy, gini index, and information gain (0.7 pt)\n",
    "  - 8-2. Random forest implementation (1.3 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First section of this task is to create three functions used to evaluate and grow the tree, which are covered in the lecture. Entropy, gini index are two main scores used for it. Information gain is the final score to choose a feature for dividing the node. Those scores are essencial for decision tree to work properly and a wrong score can lead to choosing the features that are not proper for creating a high-performing tree.\n",
    "\n",
    "- For simplicity, you will not use the **adult** dataset in this task but will use a simple **playgolf** dataset with categorical attributes.\n",
    "\n",
    "- Task 8 is a continuous task and the grade is evaluated by the result of the function. Since one function calls other functions in the task, failing to develop one function can affect the whole grade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import playgolf dataset to `playgolf`. You can find it in the homework file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "playgolf = pd.read_csv('datasets/playgolf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "playgolf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "playgolf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Subtask 1: Create a gini index function.\n",
    " - The function receives a list and calculate a gini index. The Gini Index is calculated by subtracting the sum of the squared probabilities of each class from one. \n",
    " - You should double-check the lecture slides and the examples below to make sure you made a correct function.\n",
    " - You can use collections.Counter() to count labels of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def gini(dataset):\n",
    "    \"\"\"\n",
    "    A function that calculates gini index of a given list.\n",
    "    \n",
    "    Input\n",
    "     - dataset: a list of labels.\n",
    "    Output\n",
    "     - impurity: gini index of the list.\n",
    "    \n",
    "    You do not need to keep the output name of this function, the grade only depends on the correct outputs.\n",
    "    \n",
    "    \"\"\"\n",
    "    sum_of_squared_probabilities = 0.0\n",
    "    number_of_items = len(dataset)\n",
    "    counter = Counter(dataset)\n",
    "    for item in counter:\n",
    "        item_probability = counter[item] / number_of_items\n",
    "        sum_of_squared_probabilities += (item_probability * item_probability)\n",
    "\n",
    "    impurity = 1 - sum_of_squared_probabilities\n",
    "    return impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your gini index is expected to have the following results:\n",
    "- `0.5` for `[0,0,1,1]`\n",
    "- `0.4082` for `[0,0,0,0,0,1,1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "gini([0, 0, 1, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gini([0, 0, 0, 0, 0, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report a gini score of the `Temp` attribute of **playgolf** to `gini_score` (0.2 pt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "gini_score = gini(playgolf['Temp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print your score here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "gini_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Subtask 2: Create an entropy function (0.2 pt).\n",
    " - You should double check the lecture slides and the examples below to make sure you made a correct function.\n",
    " - You can use collections.Counter() to count labels of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from math import log2\n",
    "\n",
    "\n",
    "def entropy(dataset):\n",
    "    \"\"\"\n",
    "    A function that calculates gini index of a given list.\n",
    "    \n",
    "    Input\n",
    "     - dataset: a list of labels.\n",
    "    Output\n",
    "     - impurity: entropy value of the list.\n",
    "    \n",
    "    You do not need to keep the output name of this function, the grade only depends on the correct outputs.\n",
    "    \n",
    "    \"\"\"\n",
    "    number_of_items = len(dataset)\n",
    "    counter = Counter(dataset)\n",
    "    sum_of_logs = 0\n",
    "    for item in counter:\n",
    "        item_probability = counter[item] / number_of_items\n",
    "        log_of_item_probability = log2(item_probability)\n",
    "        sum_of_logs += (item_probability * log_of_item_probability)\n",
    "    impurity = -1 * sum_of_logs\n",
    "    return impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your entropy is expected to have the following results:\n",
    "- `1.0` for `[0,0,1,1]`\n",
    "- `0.8631` for `[0,0,0,0,0,1,1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "entropy([0, 0, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "entropy([0, 0, 0, 0, 0, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report a gini score of the `Windy` attribute of **playgolf** to `entropy_score` (0.2 pt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "entropy_score = entropy(playgolf['Windy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print your score here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "entropy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Subtask 3: Create an information gain function. \n",
    "\n",
    "  - **DO NOT use entropy but only use the gini index for scores.**\n",
    "  - Check the lecture slides and the examples below to make sure you made a correct function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def information_gain(labels_start, labels_split):\n",
    "    \"\"\"\n",
    "    Calculate information gain when we have an information of label distribution before and after split operation.\n",
    "    This information gain function receives two values:\n",
    "    \n",
    "    Input:\n",
    "      - labels_start: A single list of all current labels\n",
    "        e.g.) [0,0,0,0,1,1,1,1]\n",
    "      - labels_split: A list of lists representing split \n",
    "        e.g.) [ [0,0,1,1], [1,1,0,0] ]\n",
    "    \n",
    "    Then we can calculate information gain by calculating the gini index before splitting,\n",
    "    and substract (gini index * proportion of the subset) for each list after splitting from there.\n",
    "    \n",
    "    Output:\n",
    "      - info_gain: Information gain\n",
    "    \n",
    "    You do not need to keep the output name of this function, the grade only depends on the correct outputs.\n",
    "                v\n",
    "    InfoA(D) =  âˆ‘ |Dj| / |D| * Info(Dj)\n",
    "               j=1\n",
    "\n",
    "    \"\"\"\n",
    "    sum_of_gini_split = 0.0\n",
    "    for sub_list in labels_split:\n",
    "        sub_set_proportion = labels_split.count(sub_list) / len(labels_split)\n",
    "        sum_of_gini_split += (gini(sub_list) * sub_set_proportion)\n",
    "    info_gain = gini(labels_start) - sum_of_gini_split\n",
    "    return info_gain if info_gain > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "information_gain([0, 0, 0, 0, 1, 1, 1, 1], [[0, 0, 1, 0], [1, 1, 0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "information_gain([0, 0, 0, 0, 1, 1, 1, 1], [[0, 0, 0, 0], [1, 1, 1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "information_gain([0, 0, 0, 0, 1, 1, 1, 1], [[0, 0, 1, 1], [0, 0, 1, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your information gain is expected to have the following results:\n",
    "- `0.0` for `[0,0,0,0,1,1,1,1], [[0,0,1,1],[0,0,1,1]]`\n",
    "- `0.5` for `[0,0,0,0,1,1,1,1], [[0,0,0,0],[1,1,1,1]]`\n",
    "- `0.125` for `[0,0,0,0,1,1,1,1], [[0,0,1,0],[1,0,1,1]]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have labels before and after splitting information. Use those two values to calculate information gain and report it to `info_gain_score` using your own `information_gain` function (0.2 pt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "labels_start = [1, 2, 1, 2, 2, 1, 2, 1, 3, 3, 3]\n",
    "labels_split = [[3, 3, 3], [1, 2, 1, 1], [2, 2, 1, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "info_gain_score = information_gain(labels_start, labels_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print your score here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "info_gain_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time for random forests. We will give you a basic `split` function for the algorithms you will develop. This split function receives the attributes (`X`), the label (`y`) and one `feature` (attribute) of it, and split the whole dataset based on the categories of the selected feature and return split data subsets and label subsets. Using those split values, you are going to make few functions needed for a random forest. \n",
    "\n",
    "- Note that this assignment does not make a whole working random forest but the core functions to understand the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def split(X, y, attr):\n",
    "    split_attrs = []\n",
    "    split_labels = []\n",
    "\n",
    "    for val in X[attr].unique():\n",
    "        attr_subset = []\n",
    "        label_subset = []\n",
    "\n",
    "        for idx, row in X.iterrows():\n",
    "\n",
    "            if row[attr] == val:\n",
    "                attr_subset.append(row)\n",
    "                label_subset.append(y[idx])\n",
    "\n",
    "        split_attrs.append(pd.DataFrame(attr_subset))\n",
    "        split_labels.append(label_subset)\n",
    "\n",
    "    return split_attrs, split_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the result by running the function below and also check the Windy column to understand what the function does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "split(playgolf.drop('Play Golf', axis=1), playgolf['Play Golf'], 'Windy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "playgolf['Windy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subtask 4: Now we can make a function for choosing the best feature to split, given the dataset of the node (It will be a full dataset if we run this function on the root node.). The function `best_split` receives the datasets (`X`, `y`) and the number of attributes to choose from the dataset, and returns the best feature among the chosen one and it's information gain. This process is one of the core processes of the random forest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 0.4 pt\n",
    "def best_split(X, y, num_attr):\n",
    "    \"\"\"\n",
    "    Input\n",
    "        - X: Attributes of the node.\n",
    "        - y: dataset labels.\n",
    "        - num_attr: the number of attributes that the algorithm chooses.\n",
    "    Output\n",
    "        - best_feature: The best feature in terms of information gain.\n",
    "        - best_gain: The information gain value when the dataset is split by the best feature.\n",
    "        \n",
    "    Step 1: Choose 'num_attr' column names from X.columns without allowing replacement. Use np.random.choice. \n",
    "            Assign the result to 'attributes'\n",
    "    Step 2: Set best_info_gain to zero and best_attr to None.\n",
    "    Step 3: You should iterate the attributes chosen in Step 1.\n",
    "            For each chosen attribute from Step 1, 'split' the dataset using the split function we have offered.\n",
    "            Save the split attributes and labels.\n",
    "    Step 4: Examine the information gain of the current trial. Use information_gain function you created.\n",
    "    Step 5: Compare it to the current best gain, if the new gain is higher, reset best_gain and best_feature.\n",
    "    Step 6: Return best_attr, best_info_gain.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # Step 1\n",
    "    attributes = np.random.choice(a=X.columns, size=num_attr, replace=False)\n",
    "    #print(attributes)\n",
    "    # Step 2\n",
    "    best_info_gain = 0.0\n",
    "    best_attr = None\n",
    "\n",
    "    # Step 3 - You should create a for loop and Step 4 and 5 will run inside the loop\n",
    "    for attr in attributes:\n",
    "        split_attr, split_labels = split(X=X, y=y, attr=attr)\n",
    "        # Step 4\n",
    "        current_info_gain = information_gain(y, split_labels)\n",
    "        # Step 5\n",
    "        if current_info_gain > best_info_gain:\n",
    "            best_info_gain = current_info_gain\n",
    "            best_attr = attr\n",
    "\n",
    "    # Step 6\n",
    "    return best_attr, best_info_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to find the best split of the playgolf dataset with `num_features` = 2. Report the best feature and best gain to `best_attr_playgolf` and `best_gain_playgolf` (0.4 pt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_STATE)\n",
    "best_attr_playgolf, best_gain_playgolf = best_split(X=X_train_st, y=y_train, num_attr=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# TEST YOUR RESULT HERE\n",
    "best_attr_playgolf, best_gain_playgolf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subtask 5: Now we have functions 1) to split the function based on one chosen feature (`split`) and 2) to choose the best feature to split (`find_best_split`). The next step will be to create one tree with all the information we have. Finish the function `build`. This function makes one tree of the random forest by using two previous functions. Since this function is a recursive one, it will return a complete tree, not a node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 0.5 pt\n",
    "\n",
    "def build(X, y, num_attr, tol=0.00001):\n",
    "    \"\"\"\n",
    "    Input\n",
    "        - X: Attributes of the data\n",
    "        - y: dataset labels\n",
    "        - num_attr: the number of attributes that the algorithm chooses for each node.\n",
    "        - tol: information gain tolerance value.\n",
    "    Output\n",
    "        - node: a leaf or middle node.\n",
    "        \n",
    "    Step 1: Run the best split function to get the best attributes and the best information gain for the node.\n",
    "    Step 2: Examine the best information gain value. If it is lower than the tolerance value (tol), \n",
    "            return the node with the best information gain value. The node should be a dictionary form \n",
    "            {\"type\": \"leaf\", \"gain\": the best information gain}.\n",
    "    Step 3: If the best information gain is higher, split the dataset with the chosen best attribute.\n",
    "    Step 4: Create an empty list called \"branches\" to save all the branches of the current node.\n",
    "    Step 5: For each split attributes and labels, run this 'build' function recursively and store the result\n",
    "            to the  \"branches\" list.\n",
    "    Step 6: After all the recursion process is done, return the root node with its best attribute, branch information,\n",
    "            and the best information gain.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1\n",
    "    best_attr, best_info_gain = best_split(X=X, y=y, num_attr=num_attr)\n",
    "\n",
    "    # Step 2 - Change the if condition and the return value\n",
    "    if best_info_gain < tol:\n",
    "        #print(\"Best gain is \", best_info_gain, \"and tol is \", tol)\n",
    "        return {'type': 'leaf', 'gain': best_info_gain}\n",
    "\n",
    "    # Step 3\n",
    "\n",
    "    split_attrs, split_labels = split(X, y, best_attr)\n",
    "\n",
    "    # Step 4\n",
    "    branches = []\n",
    "\n",
    "    # Step 5 - You should create a for loop following the manual\n",
    "    for attr in split_attrs:\n",
    "        tmp = build(X, y, num_attr, tol)\n",
    "        branches.append(tmp)\n",
    "    # Step 6 - Change None values to the correct values\n",
    "    return {\n",
    "        \"type\": \"node\",\n",
    "        \"best_feature\": best_attr,\n",
    "        \"branches\": branches,\n",
    "        \"value\": best_info_gain\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bulid one tree forest using the whole playgolf dataset and report the tree to the variable 'single_tree'. Set the number of attributes to three. You do not need to specify the tolerance value (0.5 pt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_STATE)\n",
    "single_tree = build(X=playgolf, y=playgolf['Play Golf'], num_attr=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "single_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subtask 6: Finally, it is time to generate a random forest with multiple trees. Complete `random_forest` function which chooses samples of the original dataset and create multiple trees forming a 'forest'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 0.4 pt\n",
    "\n",
    "def random_forest(X, y, num_tree, num_attr, tol=0.00001):\n",
    "    \"\"\"\n",
    "    Input\n",
    "        - X: Attributes of the data\n",
    "        - y: dataset labels\n",
    "        - num_tree: the number of attributes that the algorithm chooses for each node.\n",
    "        - num_attr: the number of attributes that the algorithm chooses for each node.\n",
    "        - tol: information gain tolerance value.\n",
    "    Output\n",
    "        - trees: collection of trees\n",
    "    \n",
    "    Step 1: Create a list called 'trees' to save all trees generated during the process\n",
    "    Step 2: Create a for loop iterating num_tree times.  Repeat Step 3 - 5 for 'num_tree' times in a for loop. \n",
    "            After the for loop finishes, trees list should have num_tree different trees inside.\n",
    "    Step 3: For each loop, choose indices (not values) from the dataset X of the same size, but with allowing replacement.\n",
    "            For example, you can pick [1,2,2,3,3] from the data [1,2,3,4,5]. \n",
    "            The size after random sampling should be the same as the vertical size of the dataset X.\n",
    "    Step 4: Use the same indices as X to pick the labels from y with replacement. \n",
    "            If you use DataFrame, you may need to reset the indices (reset_index).\n",
    "    Step 5: Build a tree using the subsets of X and y and save it to the list.\n",
    "    Step 6: Return 'trees'.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    trees = []\n",
    "    return trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create one random forest on playgolf dataset. Set the number of tree to three, and the number of attributes to three as well. You do not need to change tolerance value from the default one (0.4 pt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_STATE)\n",
    "forest = None  # CHANGE IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 9: Accuracy, Precision, Recall, F1-score (0.3 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start to use our original dataset again! You will evaluate the random forest and the support vector machine classifier with various performance measures you have learned besides accuracy, such as precision, recall, and F1-score, also using scikit-learn. Here we continue to use the same dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is as follows:\n",
    "\n",
    "1. Use standardized datasets (`X_train_st`, `y_train`...) throughout the task. To use the various score functions here, you need to convert the labels of `y_train` and `y_test` (`<=50K`, `>50K`) to numerical ones (0 or 1) since the score functions will not recognize the categorical labels. Create `y_train_numerical` and `y_test_numerical` with the converted labels (`<=50K` to 0, and `>50K` to 1). Refer to the previous labs. (0.1 pt)\n",
    "2. Create an instance of an SVC classifier with the **polynomial** kernel.\n",
    "3. Fit the model using the training set.\n",
    "4. Report precision score, recall score, and F1-score using the test set, and save it into the variable `recall_score_svc`, `precision_score_svc`, and `f1_score_svc`. You can find out the information about the performance measures [here](https://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-f-measure-metrics). We require you to calculate the scores using the following functions in scikit-learn: `precision_score`, `recall_score`, and `f1_score`. There is no partial point if you are correct on only some of them (0.2 pt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Subtask 1: 0.1 pt\n",
    "y_train_numerical = y_train.replace({'<=50K': 0, '>50K': 1})\n",
    "y_test_numerical = y_test.replace({'<=50K': 0, '>50K': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if you successfully replaced the values here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "y_train_numerical.unique(), y_test_numerical.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, f1_score, roc_auc_score, average_precision_score\n",
    "\n",
    "# Subtask 2: 0.2 pt\n",
    "svc = SVC(kernel='poly')\n",
    "\n",
    "svc.fit(X=X_train_st, y=y_train_numerical)\n",
    "\n",
    "y_pred = svc.predict(X=X_train_st)\n",
    "\n",
    "recall_score_svc = recall_score(y_true=y_train_numerical, y_pred=y_pred)\n",
    "\n",
    "precision_score_svc = precision_score(y_true=y_train_numerical, y_pred=y_pred)\n",
    "\n",
    "f1_score_svc = f1_score(y_true=y_train_numerical, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print three scores here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "precision_score_svc, recall_score_svc, f1_score_svc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 10: AUC / AUPRC (0.3 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will evaluate the random forest and the support vector machine classifier with various performance measures related to the ROC curve, such as the area under the ROC curve (AUC) and rea under the precision-recall curve (AUPRC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is as follows:\n",
    "\n",
    "1. Use the same dataset as the ones for Task 7 (`X_train_st` and `y_train_numerical`). AUPRC and AOC score also only recognize numerical labels.\n",
    "2. Create an instance of a random forest classifier without setting any constraint. Don't forget to set the random state to our value `RANDOM_STATE`.\n",
    "3. Fit the model on the training set.\n",
    "4. Print the accuracy on the test set to `accuracy_rf` (0.1 pt).\n",
    "4. Report AUC and AUPRC using the test set, and save it into the variable called *auc_rf, auprc_rf*. You can find out the information about the performance measures [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score) for AUC score, and [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html) for AUPRC score. AUPRC has many names, and it is supported as *average precision score* in scikit-learn. We require you to calculate the scores using the following functions: *roc_auc_score, average_precision_score*. There is no partial point if you are correct on only some of them (0.2 pt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "ran_forest = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "ran_forest.fit(X=X_train_st, y=y_train_numerical)\n",
    "y_pred = ran_forest.predict(X=X_train_st)\n",
    "\n",
    "accuracy_rf = ran_forest.score(X=X_train_st, y=y_train_numerical)\n",
    "\n",
    "auc_rf = roc_auc_score(y_true=y_train_numerical, y_score=y_pred)\n",
    "\n",
    "auprc_rf = average_precision_score(y_true=y_train_numerical, y_score=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print your scores here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "(accuracy_rf, auc_rf, auprc_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 11: Apply them together with scikit-learn (0.4 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you will try to apply the grid search using the performance measures you have tried on Task 5 and Task 6, and pick the best performing model in terms of specific performance measures.\n",
    "\n",
    "Our dataset is imbalanced, meaning that the healthy patient is dominant. Therefore, we can expect that the best model can be different, and we may also need to use AUPRC to get the most suitable model. \n",
    "\n",
    "Your task is as follows:\n",
    "\n",
    "1. Use the same dataset as the ones for Task 7 (`X_train_st` and `y_train_numerical`). AUPRC and AOC score also only recognize numerical labels.\n",
    "2. Create an instance of a kNN classifier without setting any constraint. \n",
    "3. Run grid search with a dictionary stating n_neighbors from 1 to 10, and use two different scoring measures: AUPRC (average_precision) and F1-score (f1). \n",
    "4. Put the best classifiers into the respective variable called *auprc_best_classifier* and *f1_best_classifier*. Set cv=5 for grid search cross-validation. Since grid search uses stratified k-fold inside, you should put the complete dataset, not split training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you complete the method, you can run the following line to check whether your functions are correct or not. Note that we will evaluate your functions with different data, so be careful to implement them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "grid_dictionary = {\n",
    "    'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=knn, param_grid=grid_dictionary, cv=5, scoring='average_precision')\n",
    "grid_search.fit(X=X_train_st, y=y_train_numerical)\n",
    "auprc_best_classifier1 = grid_search.best_estimator_\n",
    "f1_best_classifier1 = grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "grid_search2 = GridSearchCV(estimator=knn, param_grid=grid_dictionary, cv=5, scoring=\"f1\")\n",
    "grid_search2.fit(X=X_train_st, y=y_train_numerical)\n",
    "auprc_best_classifier2 = grid_search2.best_estimator_\n",
    "f1_best_classifier2 = grid_search2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "auprc_best_classifier = auprc_best_classifier2 if auprc_best_classifier2.n_neighbors < auprc_best_classifier1\\\n",
    "    .n_neighbors else auprc_best_classifier1\n",
    "f1_best_classifier = max(f1_best_classifier2, f1_best_classifier1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your scores here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "(auprc_best_classifier, f1_best_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 12: Task 5 implementation (0.8 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task requires you to implement the following performance measures:\n",
    " - Accuracy (0.2 pt)\n",
    " - Precision (0.2 pt)\n",
    " - Recall (0.2 pt)\n",
    " - F1-score (0.2 pt)\n",
    " \n",
    "All inputs will be the NumPy arrays, so you can use any NumPy array methods to calculate the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def compute_truth_table(truth, predicted):\n",
    "    \"\"\"\n",
    "    Computes the confusion matrix\n",
    "    Args:\n",
    "        truth: the truth/expected values\n",
    "\n",
    "        predicted: the predicted values\n",
    "\n",
    "    Returns:\n",
    "        a tuple of tp, fn, fp, tn\n",
    "    \"\"\"\n",
    "    top, bottom = confusion_matrix(y_true=truth, y_pred=predicted)\n",
    "\n",
    "    return top[0], top[1], bottom[0], bottom[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy_manual(truth, predicted):\n",
    "    tp, _, _, tn = compute_truth_table(truth, predicted)\n",
    "    return (tp + tn) /len(truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def precision_manual(truth, predicted):\n",
    "    tp,_,fp,_ = compute_truth_table(truth, predicted)\n",
    "    return tp/(tp+fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def recall_manual(truth, predicted):\n",
    "    tp, fn, fp, _ = compute_truth_table(truth, predicted)\n",
    "    return tp/(tp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def f1_score_manual(truth, predicted):\n",
    "    tp, fn, fp, _ = compute_truth_table(truth, predicted)\n",
    "    return  (2*tp)/(2*tp + fp + fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign the results of your four function on two arrays (`truth`, `predicted`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "truth =     [1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0]\n",
    "predicted = [1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "accuracy_score_manual = accuracy_manual(truth, predicted)\n",
    "precision_score_manual = precision_manual(truth, predicted)\n",
    "recall_score_manual = recall_manual(truth, predicted)\n",
    "f1_score_manual = f1_score_manual(truth, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show your results here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "(accuracy_score_manual, precision_score_manual, recall_score_manual, f1_score_manual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 13: Save models into a file using pickle (0.3 pt)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you need to pick the best model using cross-validation and deploy it as a pickle file. For this task, we will use the diabetes data that we used for Homework 1. This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage. You can find it in the homework folder.\n",
    "\n",
    "- Load the dataset into `diabetes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "diabetes = pd.read_csv(\"datasets/diabetes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split the dataset into two parts: attributes (`X`) and labels (Outcome, `y`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(data=diabetes, columns=diabetes.columns[: -1])\n",
    "y = pd.Series(data=diabetes['Outcome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is as follows:\n",
    "\n",
    "1. Create an instance of an SVC classifier without setting any constraint.\n",
    "2. Run grid search with a list of two dictionaries. In the first dictionary, you should examine 'poly' kernel, with degree = [2, 3, 4]. In the second dictionary, you should test two kernels ['linear', 'rbf'] with a list of C values [10, 100]. Use **AUPRC** as its scoring measure. Set cv=5 for grid search cross-validation. Since grid search uses stratified k-fold inside, you should put the complete dataset.\n",
    "3. Save the best classifier into the variable called `svm_best_classifier_2` and save the trained model into `model_diabetes.pickle` using pickle. When saving your model, do not specify any folder.\n",
    " - **Do not use your own specific name for the model!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "grid_dict2 = [\n",
    "    {\n",
    "        'kernel':['poly'],\n",
    "        'degree': [2,3,4]\n",
    "    },\n",
    "    {\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'C': [10, 100]\n",
    "    }\n",
    "]\n",
    "\n",
    "grid_search3 = GridSearchCV(estimator=svc, param_grid=grid_dict2, cv=5, scoring='average_precision')\n",
    "grid_search3.fit(X=diabetes, y=y)\n",
    "svm_best_classifier_2 = grid_search3.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show your classifier here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "svm_best_classifier_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = \"model_diabetes.pickle\"\n",
    "pickle.dump(svm_best_classifier_2, open(filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 14: DASH deployment (0.3 pt)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will run DASH application by using the model `model_diabetes.pickle` you exported with the project files in the `webplatform_dash` folder. Locate the model in the same folder with this jupeter notebook, and go into `webplatform_dash` folder. There you can run your own DASH application as you learned from Lab 5. Note that you should **not** move the model file into `webplatform_dash`.\n",
    "\n",
    "- Submit a screenshot with your model file into one zip file in a separate submission form for your DASH project. For the details of the DASH deployment, check out Lab 5."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e0a4af25e1bc18aecfb97b155d497d5c4ec07f6316c21bcdee370ca56222c19e"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
