{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# HW1: Data exploration and Dimensionality reduction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this assignment you will explore the dataset, handle the missing values, standardize the data and and reduce the dimensionality of the feature space. The learning outcome of this part is to know how one can pre-process a real-world dataset and prepare for a supervised or unsupervised learning task."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Student information\n",
    "Please provide your information for grading."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "STUD_SUID = 'lobe2042'\n",
    "STUD_NAME = 'Longho Bernard Che'\n",
    "STUD_EMAIL = 'lobe2042@stud.dsv.su.se'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  Grading: \n",
    "\n",
    "Total points: 4"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## OUTLINE: \n",
    "\n",
    "Data pre-processing, plotting and dimensionality reduction\n",
    "\n",
    "1. Reading the file, points: 0.1\n",
    "2. Missing Values, points: 0.2\n",
    "3. Impute with scikit-learn, points: 0.2\n",
    "4. Implement imputation, points: 0.7\n",
    "5. Plotting, total points: 0.5\n",
    "    - Boxplots, points: 0.1\n",
    "    - Pregnancies by class, points: 0.1\n",
    "    - Age by class, points: 0.2\n",
    "    - Pairplot, points: 0.1\n",
    "6. Standardization, points: 0.3\n",
    "7. Dimensionality reduction, total points: 0.5\n",
    "    - pca plotting, points: 0.3\n",
    "    - pca heatmap, points: 0.2\n",
    "8. Multi-Dimensional Scaling, total points: 1.5\n",
    "    - Multi-Dimensional Scaling, points: 0.7\n",
    "    - Multi-Dimensional Scaling with distances , points: 0.8"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `NOTE: Each function you make will be graded, so it is important to strictly follow input and output instructions stated in the skeleton code. Some of the cells have already some variables that are filled with None values or empty dataframes, you should change those nan/empty values to what is asked in the tasks (we only stored the empty values so the whole notebook can run error free). You should not delete any of the given cells as they will help us grade the assignment. Some cells ask you to uncomment some comments, please only do so if you have solved the respective task. When you are finished with implementing all the tasks, clear all outputs,  **restart the kernel**, run all cells again (make sure there is no error) and submit! Make sure that the results and figures asked are visible for us to grade. ` \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "#these are the libraries that you will need throughout the assignment. If you need anything else you can either import it here or in the cell were you are working on. \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import MDS\n",
    "RSEED = 8 \n"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/5b/lnp7nj1d66xgbj21qt5wmt_c0000gn/T/ipykernel_2960/2936672051.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mpandas\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mmatplotlib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpyplot\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mplt\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mseaborn\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0msns\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0mget_ipython\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun_line_magic\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'matplotlib'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'inline'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DATA PRE-PROCESSING, PLOTTING AND DIMENSIONALITY REDUCTION"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will use **Pima Indians Diabetes Database** that is publicly available and from UCI. However, we removed and changed some parts of the dataset for the homework evaluation, so **please use the one in the zip file provided in ilearn**.\n",
    "\n",
    "This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on specific diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dataset consists of several medical predictors (independent) variables and one target (dependent) variable, Outcome. Independent variables include the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\n",
    "\n",
    "According to the information on the data, it has eight attributes and one binary class. The brief explanation of the attributes are as follows:\n",
    "\n",
    "- Pregnancies: Number of times pregnant.\n",
    "\n",
    "- Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test.\n",
    "\n",
    "- BloodPressure: Diastolic blood pressure (mm Hg).\n",
    "\n",
    "- SkinThickness: Triceps skin fold thickness (mm).\n",
    "\n",
    "- Insulin: 2-Hour serum insulin (mu U/ml).\n",
    "\n",
    "- BMI: Body mass index (weight in kg/(height in m)^2).\n",
    "\n",
    "- DiabetesPedigreeFunction: Diabetes pedigree function.\n",
    "\n",
    "- Age: Age (years).\n",
    "\n",
    "- and we have a binary class which can be 0 (healthy) or 1 (diabetes)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## *1.* Reading the file"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### `Task: Read the dataset using pandas. Use the file called diabetes.csv that you will find on ilearn on HW1.zip under the folder datasets.`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Write your code here\n",
    "# call your dataset: data\n",
    "data = pd.read_csv(\"datasets/diabetes.csv\")\n",
    "\n",
    "data = pd.DataFrame(data=data) #change this"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# do not delete this!\n",
    "data.head(20)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# if you want to see information about the dataset:\n",
    "data.info()"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## *2.* Missing values"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### It seems like there is no null data. However, if you check zero values in the dataset, there are so many of them. \n",
    "\n",
    "### `Task: Plot a bar plot of the missing values (zero values) per attribute, that exist in the dataset. `\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# the steps are just indicative, if you want to do it your own way, please do so as long as you print the required barplot. \n",
    "# step 1: store the sum of missing values (the zero values) per attribure in a pandas Series\n",
    "# step 2: plot the missing values series as a barplot. You can use pandas to plot the series. \n",
    "\n",
    "# Write your code here\n",
    "\n",
    "def check_missing_values_columns(data_set:pd.DataFrame)->list:\n",
    "    columns_with_missing_data = list()\n",
    "    for attribute in data:\n",
    "        missing_values = (data[attribute] == 0).sum()\n",
    "        if missing_values > 0:\n",
    "            print(f\"'{attribute}' has {missing_values} missing values\")\n",
    "            columns_with_missing_data.append(attribute)\n",
    "    return columns_with_missing_data\n",
    "\n",
    "columns_with_missing_values = check_missing_values_columns(data_set=data)\n",
    "\n",
    "columns_with_missing_values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## *3.* Impute with scikit-learn"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Is it normal that people's BMI is zero? or not? What about Glucose, and Blood Pressure? You may want to change the zero values into another reasonable value, such as mean or median. The only thing that can have zero value is **pregnancies**. \r\n",
    "\r\n",
    "### **NOTE**! The outcome is your class, which is not included here in the cleaning process. So it should be left out. \r\n",
    "\r\n",
    "### `Task: Impute the missing values using the SimpleImputer from scikit-learn with strategy = 'mean'`\r\n",
    "### For the scikit-learn imputation, you can find more information [here](https://scikit-learn.org/stable/modules/impute.html).\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Scickit-learn version\n",
    "# The steps are just indicative, if you want to do it your own way, please do so as long you store in data_imputed a dataset that includes \n",
    "# both the imputed attributes and the ones that you did not impute (Pregnancies, Outcome). The original dataframe called 'data' should remain unchanged. \n",
    "  \n",
    "# step 1: create a copy of the original dataset called data_imputed\n",
    "# step 2: create a list with the names of the attributes that you will impute, call it columns\n",
    "# step 3: create a new dataset called df_part, that includes only the columns that you will impute (the ones from step 2)\n",
    "# step 4: Change the zero values in the columns to np.nan\n",
    "# step 5: define the SimpleImputer object (from sklearn) with strategy='mean' and call it imputer, fit and transform the dataset that you created in step 3 \n",
    "# step 6: convert the resulting array from step 5 into a dataframe, as column names you can pass the names of attributes of the list that you created in step 2. call it df_converted\n",
    "# step 7: in data_imputed (which is still a copy of data (the original dataset) replace the attributes you wanted to impute with the attribures in df_converted. \n",
    "\n",
    "# Important:\n",
    "# The dataframe called 'data' should remain unchanged. \n",
    "# data_imputed should contain all 8 attributes, where every attribute -except 'Pregnancies' and 'Outcome'- have imputed values \n",
    "\n",
    "\n",
    "# Write your code here\n",
    "\n",
    "\n",
    "\n",
    "data_imputed = pd.DataFrame([]) #change this"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# if you successfuly finished the imputation for task3, uncomment the following:\n",
    "#data.describe()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# if you successfuly finished the imputation for task3, uncomment the following:\n",
    "#data_imputed.describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## *4.* Implement imputation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### `Task: Make a function changing zero values into the mean of the column. In order to get the points it is necessary to write the code yourself and not use skickit-learn.`\r\n",
    "### You will impute the missing values with the mean of the column without using scikit-learn. You will store the resulting dataset in diabetes_1. diabetes_1 will not be used again after this task. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def imputation(df, columns):\n",
    "    \"\"\"\n",
    "     A function to change nan value (or zero value) to the mean of the attribute\n",
    "        \n",
    "        # the steps are just indicative\n",
    "        - Step 1: Get a part of dataframe using columns received as a parameter.\n",
    "        - Step 2: Change the zero values in the columns to np.nan\n",
    "        - Step 3: Change the nan values to the mean of each attribute (column). \n",
    "                  You could use the apply(), fillna() functions.\n",
    "        \n",
    "        Input:\n",
    "          df: A dataframe to apply imputation\n",
    "          columns: A list of columns that need to be imputed\n",
    "          \n",
    "        Output:\n",
    "          An imputed dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    #Write your code here\n",
    "\n",
    "\n",
    "    \n",
    "    return None #change this None to the df to be returned"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "diabetes_1 = imputation(data, [\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# if you successfuly finished the imputation, uncomment this\n",
    "\"\"\"\n",
    "from pandas.util.testing import assert_frame_equal\n",
    "\n",
    "try:\n",
    "    assert_frame_equal(data_imputed, diabetes_1, check_dtype=False, check_less_precise=True)\n",
    "    print(\"result: equal\")\n",
    "except:\n",
    "    print(\"result: not equal\")\n",
    "\n",
    "\"\"\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## *5.* Plotting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### a.\tBoxplots\r\n",
    "\r\n",
    "### `Task: Create 4 boxplots in 1 figure for the following attributes: Glucose, BloodPressure, Insulin, BMI`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# You can find a similar example in Lab1_Preprocessing_EDA. \n",
    "# Create 1 figure with 4 subplots. Each axes should contain the boxplot of the specified attributes and corresponding titles. Use data_imputed \n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 4)#create 1 row with 4 plots\n",
    "#Write your code here\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## b.\tPregnancies by class \r\n",
    "\r\n",
    "### `Task:  Plot the Pregnancies attribute in relation to the class (Outcome). Make a grouped barplot`\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# The steps are just indicative, if you want to do it your own way, please do so as long as you plot the asked figure\n",
    "# step 1: Store in pregnancies_by_class, a dataframe with the counts of unique values of the attribute pregnancies by class (Outcome). You can use groupby() and value_counts()\n",
    "# step 2: Plot a barplot for the pregnancies_by_class. You can use pandas to plot it.\n",
    "\n",
    "#Write your code here\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## c.\tAge by class \r\n",
    "\r\n",
    "### `Task:  Plot the Age attribute in groups of 10 years in relation to the class (Outcome)`\r\n",
    "\r\n",
    "Information about the cut function [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# step 1: Divide the age column into the following age groups: 0-19, 20-29, 30-39, 40-49, 50-59, 60-69, 70-79, 80-100 using the cut function from pandas. The new column should be called age_bins.\n",
    "# step 2: Store in age_by_class, a dataframe with the counts of unique values of the attribute age_bins by class. You can use groupby() and value_counts()\n",
    "# step 2: Plot a barplot for the age_by_class. You can use pandas to plot it.\n",
    "# step 3: **Drop the age_bins** attribute from data_imputed after you have plotted the barchart. Do not skip dropping the age_bins\n",
    "\n",
    "#Write your code here\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# do not delete this\n",
    "data_imputed.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## d.\tPairplot\r\n",
    "\r\n",
    "### `Task: Make a pairplot of all the numerical values of the dataset. You can use seaborn.`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# step1: make a pairplot, using seaborn, of all the numerical values of data_imputed. Pass hue = 'Outcome'\n",
    "# Write your code here\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## *6.* Standardization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Standardization transforms data to have a mean of zero and a standard deviation of 1. \r\n",
    "\r\n",
    "### It is a crucial step before performing PCA, since we are interested in the components that maximize the variance. \r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "### `Task: Standardize the data_imputed dataset. You can use sklearn.  `\r\n",
    "### NOTE! Outcome is the class of the dataset indicating if a patient is healthy or has diabetes. As we discussed in the lab, the class should not be included in the standardization. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# the steps are just indicative\n",
    "# step 1: Use StandardScaler to fit_transform data_imputed, excluding the class (Outcome)\n",
    "# step 3: Transform the standardized numpy matrix returned by StandardScaler into a dataframe called data_standardized.\n",
    "# step 4: Rename the columns of the dataframe with their corresponding names.\n",
    "# step 5: Store in a variable called y the attribute Outcome (your class)\n",
    "\n",
    "# Write your code here\n",
    "\n",
    "\n",
    "y = pd.Series()#change this\n",
    "data_standardized = pd.DataFrame()#change this \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# do not delete this\n",
    "data_standardized.head(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# do not delete this\n",
    "y.head(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## *7.* Dimensionality Reduction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## a. PCA plotting\r\n",
    "\r\n",
    "### `Task: Reduce the dimensionality of the standardized dataset in 2 Principal Components, with Principal Component Analysis. Print the explained variance ratio and a dataframe with all the principal components. `"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# step1: Reduce the dimensionality of the standardized dataset in 2 Principal Components, with Principal Component analysis. You can use PCA from sklearn. Use random state = RSEED\n",
    "# step2: Store the explained variance ratio in an array called explained_variance_ratio.\n",
    "# step3: Store in a dataframe called df_principal_components, the result of pca's attribute components_, with the respective attribute names.\n",
    "# step4: Plot the two principal components with colors respective to the class (Outcome).\n",
    "\n",
    "# Write your code here\n",
    "\n",
    "explained_variance_ratio =  None #change this\n",
    "df_principal_components = pd.DataFrame() #change this\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_principal_components"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "explained_variance_ratio"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## b. \tHeatmap\r\n",
    "\r\n",
    "### `Task: Use seaborn to plot the heatmap of df_principal_components. Store in a variable called attribure_contributing_the_most, which attribure contributes most to the variance of the 1st PC.`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "#Write your code here\n",
    "    \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Which attribute contributes the most in the variance of the 1st principal component? \n",
    "# store the name of the attribute here:\n",
    "\n",
    "attribute_contributing_the_most =  \" ? \""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# do not delete this\n",
    "attribute_contributing_the_most"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## *8.* Multi-Dimensional Scaling\r\n",
    "\r\n",
    "Multidimensional scaling (MDS) seeks a low-dimensional representation of the data in which the distances respect well the distances in the original high-dimensional space.\r\n",
    "In general, MDS is a technique used for analyzing similarity or dissimilarity data and it can help visualize the distances or dissimilarities between sets of objects. Examples of similarity or dissimilarity data might include the distance between pairs of cities (or planets at a particular point in time) or the similarity among groups of people (voters, patients etc). \r\n",
    "\r\n",
    "In these last two excercises we will apply Multi-Dimensional Scaling in our patient dataset using two different versions of the MDS sklearn algorithm. \r\n",
    "\r\n",
    "We will focus on the attribute **dissimilarity** of the MDS object. The attribute can be either 'euclidean' or 'precomputed'. In the former case the euclidean distance between the data points is computed by the algorithm, while in the latter case the user must themeselves compute the dissimilarities between data points and pass this to fit_transform. Your first task will be to apply MDS with dissimilarity='euclidean', while for the second MDS task to compute the distances between the points yourself (using any library you want) and pass dissimilarity='precomputed'. \r\n",
    "\r\n",
    "Please advice the sklearn page for MDS to be able to implement the above tasks: [here](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html). Make sure you understand the parameters of fit_transform and how you could use them for the tasks!\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## a. Multi-Dimensional Scaling, dissimilarity='euclidean'"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "###   `Task: Apply MDS on the data_standardized with n_components=2 and dissimilarity='euclidean'. Plot the 2 resulting coordinates with colors respective to the class label.`\r\n",
    "\r\n",
    "Note: This is very similar to how we applied sklearn's PCA on task 7!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# step 1: Initialize MDS with n_components = 2 and random_state=RSEED\n",
    "# step 2: Fit and transform the standardized dataset\n",
    "# step 3: Plot the resulting coordinates with colors respective to the class (Outcome).\n",
    "\n",
    "# write your code here\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  b. Multi-Dimensional Scaling with distances"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### `Task: Compute the pairwise distances between observations using the euclidean metric. Apply MDS on the custom matrix with the pairwise distances with n_components=2 and dissimilarity='precomputed'. Plot the results with colors respective to the class label. \r\n",
    "\r\n",
    "Note the resulting plot should look similar with the one of Task 8. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# step 1: compute the pairwise distances between observations using the euclidean metric. \n",
    "\n",
    "# One of the ways to do this is to use pdist and squareform from the scipy library (see [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html)) \n",
    "# or euclidean_distances from sklearn. In any case the returned array should be: distances ndarray of shape (n_samples_data_standardized, n_samples_data_standardized)\n",
    "\n",
    "# step 2: create an MDS object with n_components=2, random_state=RSEED, dissimilarity=\"precomputed\"\n",
    "# step 3: apply MDS on the constructed square distance matrix from step 1\n",
    "# step 4: plot the results with colors respective to the class label\n",
    "\n",
    "\n",
    "# write your code here"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `NOTE: Each function you make will be graded, so it is important to strictly follow input and output instructions stated in the skeleton code. Some of the cells have already some variables that are filled with None values or empty dataframes, you should change those nan/empty values to what is asked in the tasks (we only stored the empty values so the whole notebook can run error free). You should not delete any of the given cells as they will help us grade the assignment. Some cells ask you to uncomment some comments, please do so if you have solved the respective task. When you are finished with implementing all the tasks, clear all outputs, **restart the kernel**, run all cells again (make sure there is no error) and submit! Make sure that the results and figures asked are visible for us to grade. ` \r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# End of assignment 1"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('data-mining': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "interpreter": {
   "hash": "d152863b18335c47a24d1450da55dce515e7f4874b41ba8bc3d23bdf8dfe1916"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}